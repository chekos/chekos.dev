{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u00a1hola! \ud83d\udc4b\ud83c\udffc","text":"<p>my name is Sergio S\u00e1nchez Zavala but i go by chekos (/che/ like Guevara, /kos/ like 'costly', in lowercase) and i work as a data engineer. i'm a hip hop head policy wonk data nerd. </p> <p>i always wanted to write but i never had the time or energy or focus to do it. that's changed so here are some words.</p>"},{"location":"about/","title":"about me","text":"<p>hola again \ud83d\udc4b\ud83c\udffc I\u2019m Sergio S\u00e1nchez Zavala, a hip hop head, data nerd, and policy wonk from Tijuana, Baja California, Mexico. </p> <p>I currently work at TalkingPoints, a non-profit that helps families and schools communicate better across language barriers. </p> <p>I\u2019m also the creator of tacosdedatos.com, a platform that offers data analysis and visualization resources in Spanish.</p> tacosdedatos se encuentra en construcci\u00f3n <p>tacosdedatos inici\u00f3 como un blog en github pages. para facilitar la colaboraci\u00f3n de personas con menos experiencia en git creci\u00f3 a su propia plataforma utilizando <code>forem</code>. desafortunadamente siendo yo una sola persona no he podido darle el cuidado necesario a esta comunidad. la plataforma tambi\u00e9n atrae mucho spam de bots que crean una cuenta y escriben posts promoviendo cosas ajenas a la misi\u00f3n del espacio. todo esto vive en unos servidores en digital ocean los cuales tengo que pagar cada mes. es por eso que estoy reimaginado tacosdedatos y lo que puede ser. </p> <p>My passion is to make research clear and easy to understand, and I\u2019m working on developing accessible, open-source tools. </p> <p>I have a background in Economics and International Relations from the University of California, Davis, and I mix public policy insights with technical expertise to make a positive impact through data.</p> <p>When I\u2019m not working, I like to play with IoT projects and use open-source hardware like Raspberry Pi to bring my ideas to life. </p> <p>I believe in simple, thoughtful design and I try to make complex solutions easier to understand. Whether it\u2019s building a new application or fixing a line of code, I\u2019m always looking for ways to improve things.</p> <p>Thanks for stopping by!</p>"},{"location":"2020/02/alem%C3%A1n-alemalacra-alemalandro-alemaliya/","title":"Alem\u00e1n: Alemalacra Alemalandro Alemaliya","text":"<p>Antes de crear tacosdedatos intent\u00e9 crear un blog de Hip Hop Latino-americano donde iba a analizar de una manera cuantitativa la calidad de discos y canciones.</p>","tags":["dataviz","hip hop"]},{"location":"2020/02/alem%C3%A1n-alemalacra-alemalandro-alemaliya/#la-historia","title":"La historia","text":"<p>No lleg\u00f3 muy lejos, era en Square space y no era tan f\u00e1cil escribir notas. Bueno, no tan f\u00e1cil como lo es ahora que aprend\u00ed de blogs y sitios est\u00e1ticos en GitHub. Aqu\u00ed puedo escribir todo en mi celular y copiarlo a un archivo markdown directamente en GitHub y voil\u00e0 tengo un blog.</p> <p>El que haya fallado elblogdehiphop no significa que mi amor por el Hip Hop haya disminuido ni siquiera un poco. Seg\u00fan Spotify, mis artistas m\u00e1s reproducidos son, en orden:</p> <ol> <li>Alem\u00e1n</li> <li>La Plebada</li> <li>Gera MX</li> <li>Remik Gonz\u00e1lez</li> <li>West Gold</li> </ol> <p>Esta nota es sobre el n\u00famero uno: Alem\u00e1n.</p> <p>Alem\u00e1n ha sido uno de mis artistas favoritos desde la primera vez que lo escuch\u00e9. En aquellos tiempos yo no sab\u00eda lo que se de Hip Hop pero muy dentro de mi sent\u00eda que Alem\u00e1n era excelente en lo que hac\u00eda. No sab\u00eda que era eso todav\u00eda pero sab\u00eda que \u00e9l era uno de los grandes.</p> <p>Hoy en d\u00eda, uso la palabra flow para describir lo que diferencia Alem\u00e1n de los dem\u00e1s.</p> <p>El genio de Alem\u00e1n es saber que decir, cuando decirlo y como decirlo. No suena repetitivo, no suena forzado. En otra nota hablaremos m\u00e1s del flow de Aleman.</p> <p>Yo creo no soy el \u00fanico que piensa esto de Alem\u00e1n porque es de los pocos raperos mexicanos con m\u00e1s de un mill\u00f3n de seguidores en Spotify.</p> <p>El que sea popular ahora no significa que siempre lo fue. Alem\u00e1n comenz\u00f3 en la Mexamafia como Gera MX. Un grupo conocido por su calidad hardcore y underground. Curiosamente ambos, Alem\u00e1n y Gera MX son artistas muy exitosos el d\u00eda de hoy y aunque se adentren en el mundo del trap y sonidos m\u00e1s populares no pierden el respeto como exponentes del Hip Hop mexicano.</p> <p>En mi mente existen estas conexiones entre todos estos artistas. Alem\u00e1n con Gera MX por la Mexamafia. Pero Alem\u00e1n ahora est\u00e1 en la Homegrown con La Banda Bast\u00f6n, Yoga Fire, Fntxy, Cozy Cuz, Mike D\u00edaz, Dee. Dee es parte de Hood P con MOF. Mike D\u00edaz es Neverdie con el Eptos. La Banda Bast\u00f6n es Vieja Guardia. Al Gera lo relaciono con Charles Ans pero Charles es Anestesia. Charles Ans tiene rolas con Taxi Dee (el nombre que Fntxy usa cuando produce). Fntxy ahora tiene el grupo La Plebada junto a Cozy Cuz quien va Bobby Bass cuando produce. Bobby Bass comenz\u00f3 a agarrar m\u00e1s tracci\u00f3n cuando comenz\u00f3 a trabajar con Alem\u00e1n.</p> <p>Todos se conectan. En mi mente, por lo menos. Quer\u00eda saber si los datos respaldaban mis pensamientos.</p> <p>Hace unas semanas encontr\u00e9 esta herramienta: http://labs.polsys.net/playground/spotify/</p> <p>La herramienta utiliza la API de Spotify para crear una red de artistas relacionados hasta dos niveles de separaci\u00f3n. Es decir, cuando yo escribo Alem\u00e1n en la caja de texto la herramienta va y busca todos los artistas relacionados a Alem\u00e1n (nivel uno) y tambi\u00e9n busca los artistas relacionados esos artistas (nivel dos).</p> <p>As\u00ed se ve la red de Alem\u00e1n </p> <p>La herramienta tiene la opci\u00f3n de descargar los datos. Uno de los atributos de esos datos son las IDs \u00fanicas que Spotify le asigna a cada artista. Con estas IDs puedes utilizar la API de Spotify para obtener m\u00e1s informaci\u00f3n de cada artista como su \u00edndice de popularidad, cu\u00e1ntos seguidores tienen, sus canciones m\u00e1s populares y mucho m\u00e1s.</p> <p>Justo eso fue lo que hice para crear una visualizaci\u00f3n diferente. Sabiendo que el \"universo\" de mis datos es artistas relacionados a Alem\u00e1n hasta dos niveles de separaci\u00f3n puedo hacer preguntas como:</p> <ol> <li>\u00bfc\u00f3mo se compara la popularidad de Alem\u00e1n con la de artistas relacionados?</li> <li>\u00bfde qu\u00e9 g\u00e9neros musicales vienen \u00e9stos artistas?</li> <li>\u00bfcu\u00e1ntos artistas relacionados a Alem\u00e1n tienen m\u00e1s de un mill\u00f3n de seguidores?</li> </ol> <p>M\u00e1s que todo esto, quer\u00eda una manera f\u00e1cil de explorar estos datos.</p> <p>El resultado fue este Observable Notebook: https://observablehq.com/@chekos/aleman-beeswarm-plot-using-spotify-data</p>","tags":["dataviz","hip hop"]},{"location":"2020/02/alem%C3%A1n-alemalacra-alemalandro-alemaliya/#la-visualizacion","title":"La visualizaci\u00f3n","text":"<p>Hay solo 5 artistas (+ Alem\u00e1n) con m\u00e1s de un mill\u00f3n de seguidores en Spotify:</p> <ol> <li>Cartel de Santa 3.94M</li> <li>El Komander 1.46M</li> <li>Molotov 1.42M</li> <li>Pante\u00f3n Rococo 1.28M</li> <li>Beret 1.15M</li> <li>Alem\u00e1n 1.08M</li> </ol> <p>Muchos son artistas de rap y hip-hop pero tambi\u00e9n hay artistas de pop, rock en espa\u00f1ol, reggea y ska.</p> <p>Alem\u00e1n est\u00e1 entre los m\u00e1s populares de este universo lo cual me estoy tomando la libertad de etiquetar como positivo. De alguna manera, este artista underground que lleg\u00f3 a esta altura le est\u00e1 abriendo la puerta a todos estos dem\u00e1s artistas con menos popularidad. M\u00ednimo, Spotify los identifica como artistas relacionados y tal vez aparezcan en una de esas listas de reproducci\u00f3n automatizadas juntos \ud83e\udd37\ud83c\udffb\u200d\u2642\ufe0f</p> <p>Share on  Share on  Share on </p>","tags":["dataviz","hip hop"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/","title":"Haciendo datos abiertos m\u00e1s accesibles con datasette","text":"<p>California recientemente liber\u00f3 datos sobre las detenciones hechas por oficiales de las 8 agencias m\u00e1s grandes del estado. Estos datos cubren los meses de julio a diciembre del 2018. Esta fue la primera ola de divulgaci\u00f3n de datos que entrar\u00e1 en vigencia en los a\u00f1os siguientes. Los datos cubrieron m\u00e1s de 1.8 millones de paradas en todo el estado. Si bien este es un paso en la direcci\u00f3n correcta, un solo archivo <code>.csv</code> de alrededor de 640 megabytes con m\u00e1s de 1.8 millones de filas y m\u00e1s de 140 columnas podr\u00eda ser intimidante para algunas personas que se beneficiar\u00edan de la exploraci\u00f3n de estos datos: l\u00edderes locales, periodistas, activistas y organizadores, por nombrar algunos.</p> <p>El tercer principio de la carta internacional de datos abiertos es que los datos deben ser accesibles y utilizables.</p> <p>Proporcionar los datos es el primer paso, pero para que sean utilizables y accesibles para la mayor\u00eda de las personas, no podemos simplemente publicar datos; debemos tener en cuenta la experiencia del usuario y desarrollar formas de facilitar la exploraci\u00f3n y el uso de dichos datos.</p> <p>En este tutorial, comparto una forma de hacerlo: usando datasette, una \"herramienta de Python para explorar y publicar datos. Ayuda a las personas a tomar datos de cualquier forma o tama\u00f1o y publicarlos como un sitio web interactivo y explorable y con una API acompa\u00f1ante\".</p> <p>Tomamos los datos de detenci\u00f3n policial lanzados recientemente por California, los limpiamos y transformamos a como sea necesario, y desplegamos una instancia de datasette en Heroku.</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#prologo","title":"pr\u00f3logo","text":"<p>La idea de explorar este conjunto de datos vino al escuchar el episodio del 3 de marzo del 2020 del podcast Pod Save the People. (minuto 4:12)</p> <p>donde mencionan un art\u00edculo de The Appeal:</p> <p>Boudin will announce a second directive today, also reviewed by The Appeal, on what are known as pre-textual stops, in which an officer stops someone for a minor offense or infraction, such as a traffic violation, in order to conduct an unrelated search for items like guns or drugs. According to the new policy, the DA's office will not prosecute possession of contraband cases when the contraband was collected as a result of an infraction-related stop, \"where there is no other articulable suspicion of criminal activity.\" Any deviations from the policy should be made in writing and require approval from the DA or a chief of the criminal division. Additionally, the ban includes cases in which a person consented to a search \"because of the long-standing and documented racial and ethnic disparities in law enforcement requests for consent to search,\" according to the directive.  - https://theappeal.org/san-francisco-da-to-announce-sweeping-changes-on-sentencing-policy-and-police-stops/</p> <p>En el episodio, Sam Sinyangwe menciona que las personas negras y de color est\u00e1n siendo detenidas y registradas a tasas m\u00e1s altas, y que muchos de estos registros son lo que se conocen como 'registros consensuales', lo que significa que la polic\u00eda no informa ninguna justificaci\u00f3n para registrar a la persona mas que preguntar a esa persona si puede registrarlos y que la persona presuntamente da su consentimiento.</p> <p>Esta gran disparidad racial es desgarradora pero no es sorpresa.</p> <p>Cuando comenc\u00e9 a trabajar con el conjunto de datos me pareci\u00f3 un tanto \u00edncomodo, no era f\u00e1cil. El tama\u00f1o del conjunto de los datos hace que sea d\u00edficil trabajar con \u00e9l para aquellas personas que no analizan datos program\u00e1ticamente; ya sea con recursos pagados como stata y sas o gratuitos como python y R.</p> <p>Esta informaci\u00f3n no esta dise\u00f1ada para ser explorada f\u00e1cilmente pero existen herramientas que pueden ayudar con eso.</p> <p>\ud83d\udca1Nota: AB-953, el proyecto de ley que requiere que las agencias reporten todos los datos sobre paradas al Procurador General, no requiere que los datos sean f\u00e1cilmente \"explorables\" o \"accesibles\". El proyecto de ley requiere que los datos sean recopilados y reportados al Procurador General. Este conjunto de datos tal como es sirve para ese prop\u00f3sito.</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#sobre-los-datos","title":"sobre los datos","text":"<p>El conjunto de datos est\u00e1 compuesto por un archivo <code>.csv</code> de 641.4 MB que contiene 1.8 millones de filas y 143 columnas. Cada parada tiene un <code>DOJ_RECORD_ID</code> \u00fanico y cada persona detenida tiene un <code>PERSON_NUMBER</code>. En total, hay 1,708,377 paradas en este conjunto de datos que involucran a 1,800,054 personas.</p> <p>El conjunto de datos incluye informaci\u00f3n b\u00e1sica sobre cada parada (como duraci\u00f3n, hora del d\u00eda, ciudad m\u00e1s cercana, nombre de la agencia), informaci\u00f3n demogr\u00e1fica percibida (raza / etnia, g\u00e9nero, edad, discapacidad), as\u00ed como informaci\u00f3n sobre el motivo de detener, buscar, incautar, acciones tomadas durante la detenci\u00f3n, contrabando o evidencia encontrada, etc. Para obtener informaci\u00f3n m\u00e1s detallada sobre el conjunto de datos, se puede leer el archivo README que reuni\u00f3 el Departamento de Justicia y el Informe Anual 2020.</p> <p>\ud83d\udca1Nota: Los datos reflejan la percepci\u00f3n del oficial. Son datos demogr\u00e1ficos percibidos, es decir, si pareces hispano, te marcan como hispano. Si pareces de 27 a\u00f1os te marcan de 27 a\u00f1os. Este es otro problema para otra ocasi\u00f3n pero hay que tenerlo en mente cuando trabajamos con estos datos.</p> <p>Esta es la Ola I de una serie de datos de la Racial and Identity Profiling Act (RIPA) que se lanzar\u00e1n en los a\u00f1os siguientes. Los datos publicados sobre esta ola pertenecen a las 8 agencias de aplicaci\u00f3n de la ley (LEA, por sus siglas en ingl\u00e9s) m\u00e1s grandes de California (las que emplean a m\u00e1s de 1,000 oficiales). Estas LEA (y su parte de las observaciones totales) son:</p> Law enforcement agency (LEA) N % California Highway Patrol\u200a \u200a1,033,421 57.4% Los Angeles Police Department \u200a 336,681 18.7% Los Angeles Sheriff Department \u200a \u200a 136,635 7.59% San Diego Police Department \u200a \u200a 89,455 4.97% San Bernardino Sheriff Department\u200a \u200a 62,433 3.47% San Francisco Police Department\u200a \u200a 56,409 3.14% Riverside Sheriff Department \u200a \u200a 44.505 2.47% San Diego Sheriff Department \u200a \u200a 40,515 2.25% <p>README | Informe Anual 2020</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#sobre-datasette","title":"sobre datasette","text":"<p>Datasette es una herramienta para explorar y publicar datos. Ayuda a las personas a tomar datos de cualquier forma o tama\u00f1o y publicarlos como un sitio web interactivo y explorable y la API que lo acompa\u00f1a. Datasette est\u00e1 dirigido a periodistas de datos, conservadores de museos, archiveros, gobiernos locales y cualquier otra persona que tenga datos que deseen compartir con el mundo. Es parte de un ecosistema m\u00e1s amplio de herramientas y complementos dedicados a hacer que trabajar con datos estructurados sea lo m\u00e1s productivo posible .  - datasette.readthedocs.io</p> <p>datasette es el motor que impulsa este proyecto. En resumen, toma una base de datos sqlite y crea un sitio web interactivo y explorable y API que lo acompa\u00f1a. Para preparar los datos, utilizamos <code>csvs-to-sqlite</code>, otra herramienta del ecosistema datasette que toma archivos CSV y crea bases de datos sqlite a partir de ellos.</p> <p>Puedes encontrar ejemplos de datasettes p\u00fablicos en el wiki del repositorio en GitHub. Aqu\u00ed se encuentra uno siriviendo los conjuntos de datos publicados por FiveThirtyEight (puedes encontrarlos en su repositorio de GitHub): https://fivethirtyeight.datasettes.com/</p> <p>La p\u00e1gina principal muestra la licencia y la fuente de la base de datos. Tambi\u00e9n provee acceso r\u00e1pido a algunas de sus tablas.</p> <p></p> <p>Si haces clic en el nombre de la base de datos, proporcionar\u00e1 una vista de todas sus tablas y un cuadro de texto para que ejecute consultas SQL personalizadas. Esta es una de las caracter\u00edsticas que hace que datasette sea una herramienta tan poderosa.</p> <p></p> <p>Si haces clic en una de las tablas, ver\u00e1s las facetas sugeridas, un bot\u00f3n de filtro f\u00e1cil de usar, un enlace JSON y CSV que proporcionar\u00e1 la tabla como uno de esos formatos (esto significa que puede usar esto como API) y una descripci\u00f3n de la tabla. En este caso, la descripci\u00f3n contiene una tabla HTML con encabezados y definiciones.</p> <p></p> <p>Si haces clic en el bot\u00f3n \"View and edit SQL\", volver\u00e1s a tener acceso a un cuadro de texto para escribir sus propias consultas.</p> <p></p> <p>datasette es parte de un ecosistema de herramientas y plugins. Puedes agregar el plugin datasette-vega (repo) para agregar visualizaciones interactivas de la tabla (hechas con altair)</p> <p></p> <p>As\u00ed es como se ve un sitio web b\u00e1sico de datasette pero datasette es altamente personalizable. Tomemos, por ejemplo, el datasette de The Baltimore Sun para que las personas exploren los registros de salarios p\u00fablicos que se actualizan anualmente.</p> <p></p> <p>Esto esta corriendo datasette detr\u00e1s de c\u00e1maras pero The Baltimore Sun agrego sus propias plantillas y archivos CSS y JS. Para aprender m\u00e1s sobre como personalizar datasette visita https://datasette.readthedocs.io/en/stable/custom_templates.html</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#paso-paso-a-pasito","title":"paso, paso a pasito","text":"<p>Esta secci\u00f3n es una descripci\u00f3n m\u00e1s t\u00e9cnica del proyecto. La preparaci\u00f3n y despliegue de datos con datasette es bastante sencillo y se puede dividir en tres fases:</p> <ul> <li>Adquirir los datos</li> <li>Preparar los datos</li> <li>Servir los datos</li> </ul>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#adquiriendo-los-datos","title":"adquiriendo los datos","text":"<p>Los datos se obtuvieron del sitio web de Open Justice del Departamento de Justicia de California: https://openjustice.doj.ca.gov/data. El sitio web proporciona un enlace para descargar los datos (por lo menos desde 5 de mayo de 2020, cuando lo descargu\u00e9 para este proyecto). Para obtener m\u00e1s informaci\u00f3n sobre los datos en s\u00ed, puede leer la secci\u00f3n Acerca de los datos en el repositorio de GitHub del proyecto.</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#preparando-los-datos","title":"preparando los datos","text":"<p>El CSV original es de mas de 640 MB as\u00ed que el primer paso es dividirlo en 15 archivos para que cada uno pueda subirse a GitHub en este repositorio.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nTHIS_FILE = Path(__file__)\nTHIS_DIR = THIS_FILE.parent\nEXTERNAL_DATA = THIS_DIR.joinpath(\"./../../data/external/\")\nINTERIM_DATA = THIS_DIR.joinpath(\"./../../data/interim/\")\n\nFULL_DB = \"RIPA Stop Data 2018.csv\"\ndata = pd.read_csv(EXTERNAL_DATA.joinpath(FULL_DB))\n\ndfs = np.array_split(data, 15)\nfor idx,df in enumerate(dfs):\n    print(f\"Saving dataset #{idx}\")\n    df.to_csv(INTERIM_DATA.joinpath(f\"ripa-2018-part-{idx}.csv\"), index = False)\n</code></pre> <p>- https://github.com/chekos/RIPA-2018-datasette/blob/fea53ce60ae43f0d7b0dd130109a01496f08e20a/src/data/split_data.py#L1-L16</p> <p>\ud83d\udca1\u00bfPor qu\u00e9? Nom\u00e1s por si desaparece misteriosamente de la fuente original...</p> <p>Sin embargo, debido a que el conjunto de datos es demasiado grande para servir como una sola tabla (1,8 millones de filas por 143 columnas), tambi\u00e9n se dividi\u00f3 en tablas m\u00e1s peque\u00f1as. Esto significa que tomamos variables relacionadas (basadas en sus sufijos) y las extrajimos en sus propias tablas. Por ejemplo, variables relacionadas con el g\u00e9nero como <code>G_FULL</code>,<code>G_MALE</code>, <code>G_FEMALE</code>,<code>G_TRANSGENDER_MAN</code>, <code>G_TRANSGENDER_WOMAN</code>,<code>G_GENDER_NONCOMFORMING</code>, y <code>G_MULTIGENDER</code> se extrajeron de la tabla \"principal \"y se agregaron a una tabla de g\u00e9nero en la base de datos.</p> <p>Estas se pueden volver a unir a la tabla principal utilizando un UNIQUE_ID asignado a ellas.</p> <p>Este script es demasiado grande para incluir en este blog sin crear una distracci\u00f3n inecesaria pero puedes ver las 91 l\u00edneas de c\u00f3digo aqu\u00ed: https://github.com/chekos/RIPA-2018-datasette/blob/master/src/data/break_down_database.py</p> <p>A cada observaci\u00f3n o fila de este conjunto de datos se le asigna un <code>DOJ_RECORD_ID</code> y un<code>PERSON_NUMBER</code>. Estos son exclusivos de la parada y las personas detenidas, respectivamente. Esto significa que podr\u00edamos combinarlos para crear un 'ID \u00daNICO' para cada fila que podr\u00edamos usar para unir tablas. Sin embargo, esto termina siendo una cadena de 22 caracteres que es innecesariamente grande. Para facilitar las cosas, a cada fila se le asigna una identificaci\u00f3n num\u00e9rica que comienza en 1,000,000. Comenzar en un mill\u00f3n es completamente arbitrario, podr\u00edamos haber comenzado en cero, pero debido a que hay 1.8 millones de filas, tomamos la decisi\u00f3n de que cada identificaci\u00f3n num\u00e9rica sea de siete d\u00edgitos. Este <code>UNIQUE_ID</code> num\u00e9rico nos permite unir tablas juntas y no es una gran adici\u00f3n a la base de datos en t\u00e9rminos de memoria.</p> <p>Una vez que se ha creado este <code>ID_\u00daNICO</code>, podemos extraer columnas de la tabla \"principal\" en sus propias tablas y guardarlas como archivos CSV individuales con la certeza de que cada observaci\u00f3n puede coincidir en estas nuevas tablas.</p> <p>Luego usamos <code>csvs-to-sqlite</code> para crear una base de datos sqlite donde cada CSV es una tabla. En este paso, tambi\u00e9n incluimos el archivo Appendix B Table 3.csv obtenido tambi\u00e9n del sitio web del DOJ y cualquier otra tabla complementaria que podr\u00edamos haber creado para acompa\u00f1ar el conjunto de datos.</p> <pre><code>csvs-to-sqlite data/processed/*.csv \"data/external/Appendix B Table 3.csv\" datasette/ripa-2018-db.db\n</code></pre>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#sirviendo-los-datos","title":"sirviendo los datos","text":"<p>Despu\u00e9s de preparar los datos y crear una base de datos sqlite usamos datasette para servirlos como un sitio web interactivo y su API. Esto es tan f\u00e1cil como ejecutar</p> <pre><code>datasette ripa-2018.db\n</code></pre> <p>Pero para este proyecto personalizamos nuestro datasette un poco.</p> <p>Incluimos un t\u00eetulo, una descripci\u00f3n, la url de la fuente de los datos y archivos CSS y JS. Puedes explorar el archivo <code>metadata.json</code> en el repositorio datasette/metadata.json para ver que m\u00e1s hicimos.</p> <p>Tambi\u00e9n inclu\u00edmos lo que datasette llama canned queries, lo que ser\u00eda \"consultas almacenadas\" (?). Estas son consultas de SQL inclu\u00eddas en tu instancia que aparecen al fondo de tu p\u00e1gina principal y tiene su propio URL para facilitar el acceso. Estas consultas las incluimos porque muestran informaci\u00f3n \u00fatil y/o interesante de los datos. Algunas de ellas son consultas que computan informaci\u00f3n especifica publicada en el reporte anual del 2020.</p> <p>Tambi\u00e9n modificamos algunas plantillas de datasette, especificamente <code>base.html</code> y <code>query.html</code>. La primera fue modificada para incluir metadatos en la etiqueta <code>&lt;head&gt;</code> (descripci\u00f3n del sitio, por ejemplo, para cuando se comparta un enlace). La segunda fue modificada para incluir un bot\u00f3n debajo del cuadro de texto donde escribes tus consultas de SQL. Este bot\u00f3n sirve para facilitar las sugerencias de consultas para el repositorio. Al hacer clic, se abre otra ventana en tu navegador que crea un nuevo issue en github con la consulta de SQL que acabas de ejecutar.</p> <p>Tambi\u00e9n cambiamos algunas opciones para datasette:</p> <ol> <li><code>default_page_size:50</code> - Muestra solo 50 resultados por p\u00e1gina</li> <li><code>sql_time_limit_ms:30000</code> - Un l\u00edmite de 30 segundos para ejecutar consultas (es el l\u00edmite de tiempo de Heroku)</li> <li><code>facet_time_limit_ms:10000</code> - El tiempo l\u00edmite que datasette deber\u00eda usar calculando una posible faceta de tu tabla (el default es de 200ms pero nuestro conjunto de datos es tan grande que lo expandimos a 10 segundos)</li> </ol> <p>El c\u00f3digo por ejecutar es:</p> <pre><code>datasette ripa-2018-db.db \\\\\n  -m metadata.json \\\\\n  --extra-options=\"--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000\"\n</code></pre>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#desplegando-en-heroku","title":"desplegando en heroku","text":"<p>Aqu\u00ed hay una descripci\u00f3n general de alto nivel del proceso</p> <p></p> <p>Hasta ahora, utilizamos un c\u00f3digo de Python para procesar nuestros datos (creando un <code>UNIQUE_ID</code> y desglosando el conjunto de datos original con 143 columnas en varios archivos<code>.csv</code> m\u00e1s peque\u00f1os) y tambi\u00e9n utilizamos <code>csvs-to-sqlite</code> para construir nuestra base de datos <code>ripa-2018-db.db</code> de esos archivos<code>.csv</code>. Hasta ahora, hemos estado interactuando con nuestro datasette ejecutando el comando <code>datasette ripa-2018-db.db</code> que ejecuta un servidor local. Para que nuestro datasette est\u00e9 disponible para el mundo, tenemos que implementarlo en l\u00ednea y, por suerte, es muy, muy f\u00e1cil de hacer, ya que datasette ya incluye un comando de publicaci\u00f3n.</p> <p>Con datasette puedes publicar directamente a Heroku, Google Cloud Run, o Fly (por lo menos desde la versi\u00f3n 0.42 que es la que utilizamos en este proyecto). Debido a que tengo experiencia previa en la implementaci\u00f3n de heroku, me pareci\u00f3 la m\u00e1s f\u00e1cil de las tres, pero todas son excelentes opciones y la documentaci\u00f3n es f\u00e1cil de seguir.</p> <p>Implementar en Heroku fue tan f\u00e1cil como ejecutar casi el mismo comando utilizado para servir nuestro datasette localmente.</p> <pre><code>datasette publish heroku ripa-2018-db.db \\\\\n  --name ripa-2018-db \\\\\n  -m metadata.json \\\\\n  --extra-options=\"--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000\"\n</code></pre> <p>\ud83d\udca1 Nota la bandera <code>--name</code> que espcif\u00edca el nombre de nuestra aplicaci\u00f3n en Heroku. Esto significa que la publicar\u00e1 a ripa-2018-db.herokuapp.com y sobreescribir\u00e1 cualquier aplicaci\u00f3n previa ah\u00ed.</p> <p>Para ejecutar esto, necesitar\u00e1 una cuenta heroku e instalar la herramienta de l\u00ednea de comandos de heroku. Datasette (usando el la herramienta de l\u00ednea de comandos) te pedir\u00e1 que inicies sesi\u00f3n y abrir\u00e1 una ventana del navegador para que lo haga. Despu\u00e9s de iniciar sesi\u00f3n, se encargar\u00e1 del resto.</p> <p>\u00a1Listo! En este punto, hemos publicado con \u00e9xito nuestro datasette en heroku y podemos visitar ripa-2018-db.herokuapp.com o ripa-2018.datasettes.cimarron.io/</p> <p></p> <p>Este ser\u00e1 el final de este proceso para muchas personas. Adquirimos y transformamos con \u00e9xito algunos datos y los desplegamos en la nube para que otros puedan explorar e interactuar con ellos. Tambi\u00e9n hemos incluido una descripci\u00f3n HTML que describe nuestro datasette para los usuarios e incluso algunas consultas almacenadas para que las personas reproduzcan algunos de los hechos publicados en el Informe Anual 2020.</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#automatizando-todo","title":"automatizando todo","text":"<p>Si bien los datos subyacentes que estas publicando no cambian con frecuencia, es posible que desees automatizar la implementaci\u00f3n de su instancia de datasette por muchas razones. Por ejemplo, cuando comenc\u00e9 este proyecto, datasette estaba en la versi\u00f3n 0.40 y en el momento en que escribo esta publicaci\u00f3n est\u00e1 en la versi\u00f3n 0.42. Para la mayor\u00eda de las versiones no necesitar\u00e1s actualizar tu instancia de datasette, pero la versi\u00f3n 0.41 inclu\u00eda la capacidad de crear p\u00e1ginas personalizadas (changelog).</p> <p>For example, adding a template file called <code>templates/pages/about.html</code> will result in a new page being served at <code>/about</code> on your instance.</p> <p>Esto significa que podemos agregar mucho m\u00e1s contexto en nuestra instancia para los usuarios. Tal vez incluya una gu\u00eda paso a paso para ayudar a las personas a contribuir al proyecto, otros enlaces \u00fatiles o una p\u00e1gina simple para presentarse para que las personas que usan estos datos aprendan un poco m\u00e1s sobre ti.</p> <p>Tambi\u00e9n es posible que desee incluir m\u00e1s consultas almacenadas o corregir un error ortogr\u00e1fico en su descripci\u00f3n. Cualquiera sea la raz\u00f3n, la implementaci\u00f3n autom\u00e1tica es f\u00e1cil de lograr. En definitiva, todo lo que necesita es ejecutar <code>datasette heroku publishing</code>, que es un caso de uso perfecto para las GitHub Actions.</p> <p>\ud83d\udea8 Advertencia / Nota: Esta pr\u00f3xima parte se vuelve mucho m\u00e1s t\u00e9cnica r\u00e1pidamente. GitHub Actions es un tema m\u00e1s avanzado. Si no necesitas / deseas actualizar su instancia de datasette recientemente implementada regularmente, no recomendar\u00eda pensar en las GitHub Actions por el momento.</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#github-actions","title":"GitHub Actions","text":"<p>GitHub Actions help you automate your software development workflows in the same place you store code and collaborate on pull requests and issues. You can write individual tasks, called actions, and combine them to create a custom workflow. Workflows are custom automated processes that you can set up in your repository to build, test, package, release, or deploy any code project on GitHub.  - https://help.github.com/en/actions/getting-started-with-github-actions/about-github-actions</p> <p>No entraremos muy a fondo en las acciones de GitHub en esta publicaci\u00f3n. Lo que necesitas saber es que con las GitHub Actions puede ejecutar tareas en paralelo o en secuencia, y estos se desencadenan por eventos de GitHub como empujar tu c\u00f3digo a una branch, abrir una pull request, comentar en un issue o una combinaci\u00f3n de muchos.</p> <p>Estos existen en sus repositorios de GitHub en <code>.github/workflows/</code> generalmente en forma de archivos yaml. La estructura b\u00e1sica es la siguiente.</p> <pre><code>name: Example of simple GitHub Action\n\non:\n  push:\n    branches: [master]\n\njobs:\n  say-hi:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Echo Hello World\n        run: |\n          echo \"Hello World!\"\n</code></pre> <p>Esta acci\u00f3n se activar\u00e1 cada vez que haya un impulso en su rama maestra y ejecutar\u00e1 el trabajo <code>say-hi</code> que se ejecuta en una m\u00e1quina virtual (VM) ubuntu disponible. En esa VM ejecutar\u00e1 el c\u00f3digo <code>echo \"Hello World!\"</code></p> <p>Puede ver los resultados / registros de sus acciones en la pesta\u00f1a Actions de tu repositorio.</p> <p></p> <p>Podr\u00edas cambiar facilmente <code>echo \"Hello World!\"</code> con</p> <pre><code>datasette publish heroku ripa-2018-db.db \\\\\n  --name ripa-2018-db \\\\\n  -m metadata.json \\\\\n  --extra-options=\"--config default_page_size:50 --config sql_time_limit_ms:30000 --config facet_time_limit_ms:10000\"\n</code></pre> <p>o mejor a\u00fan puedes guardar tu c\u00f3digo en un script de bash llamado <code>heroku_deploy.sh</code> y ejecutar <code>sh heroku_deploy.sh</code> como ejecutar\u00edas <code>echo \"Hello World!\"</code></p> <p>Afortunadamente, Heroku ya est\u00e1 instalado en nuestro corredor de la GitHub Action (ubuntu-latest), por lo que todo lo que tenemos que hacer es iniciar sesi\u00f3n, instalar el complemento <code>heroku-builds</code> y ejecutar nuestro script <code>heroku_deploy.sh</code>.</p> <p>\ud83d\udca1 Aprend\u00ed sobre la necesidad de instalar <code>heroku-builds</code> despu\u00e9s de que las acciones de GitHub fallaran un par de veces. No estoy seguro de que est\u00e9 documentado en datasette o en la documentaci\u00f3n de GitHub Actions.</p> <p>Hasta ahora, nuestra GitHub Action se ve as\u00ed:</p> <pre><code>name: Example of simple GitHub Action\n\non:\n  push:\n    branches: [master]\n\njobs:\n  publish-to-heroku:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Publish to Heroku\n        run: |\n          heroku container:login &amp;&amp; heroku plugins:install heroku-builds &amp;&amp; sh heroku_deploy.sh\n</code></pre> <p>Esto, sin embargo, a\u00fan no est\u00e1 listo para funcionar. Para automatizar el inicio de sesi\u00f3n, debe incluir la variable de entorno <code>HEROKU_API_KEY</code>. Esto es posible mediante el uso de GitHub Secrets.</p> <p>Para crear tu API key necesitas tener instalada la herramienta de heroku para la l\u00ednea de comando localmente. Ejecuta el comando <code>heroku authorizations:create</code> y a\u00f1adela a tu repositorio en Settings &gt; Secrets. Ll\u00e1malo <code>HEROKU_API_KEY</code>.</p> <p>Para que su GitHub Action tenga acceso a ella, debe agregar la siguiente l\u00ednea a su archivo yaml</p> <pre><code>name: Example of simple GitHub Action\n\non:\n  push:\n    branches: [master]\n\njobs:\n  publish-to-heroku:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Publish to Heroku\n        env: ${{ secrets.HEROKU_API_KEY }}\n        run: |\n          heroku container:login &amp;&amp; heroku plugins:install heroku-builds &amp;&amp; sh heroku_deploy.sh\n</code></pre> <p>\u00a1Listo! Ahora, cada vez que empujes c\u00f3digo a tu repositorio en la branch \"master\", implementar\u00e1 una nueva versi\u00f3n de tu datasette. Esto significa que puede actualizar su <code>metadata.json</code>, por ejemplo, con una nueva consulta almacenada o agregar una nueva p\u00e1gina a tus<code>templates/pages</code>.</p> <p>Para este proyecto, tengo algunos pasos adicionales (pasos de GitHub Action, es decir) que procesan los datos, construyen la base de datos sqlite3 y la despliegan en heroku cada vez que hay un nuevo \"commit\" en \"master\". Tambi\u00e9n incluyo un par de scripts de Python que leen un archivo yaml separado donde tengo todas las consultas almacenadas y las agrego a un <code>updated_metadata.json</code>. Esto es para mantener las cosas un poco m\u00e1s limpias, cada consulta almacenada tiene un t\u00edtulo, html_description y una larga consulta SQL; Es m\u00e1s f\u00e1cil de mantener como un archivo separado, en mi opini\u00f3n.</p> <p>Puedes ver el archivo yaml de mi GitHub Action aqu\u00ed https://github.com/chekos/RIPA-2018-datasette/blob/master/.github/workflows/main.yml</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#bonus","title":"bonus","text":"<p>using github issues to suggest\u00a0queries</p> <p>Si visitas  y ejecutas una consulta, notar\u00e1s el bot\u00f3n Submit in GitHub. <p></p> <p>Esto se hace mediante un ajuste de la plantilla <code>query.html</code>. Datasette usa <code>jinja2</code> y en realidad est\u00e1 pasando la consulta SQL como un par\u00e1metro de consulta de URL, lo que significa que puede acceder a \u00e9l usando <code>request.query_string.split('sql =')[- 1]</code></p> <p>Ya que tienes acceso a la consulta es facil crear un enlace directo a un issue nuevo en tu repo</p> <p>{% raw %}</p> <pre><code>{% set link_to_new_issue = \"&lt;https://GitHub.com/&gt;&lt;YOUR_USERNAME&gt;/&lt;YOUR_REPO&gt;/issues/new?title=Query+suggestion&amp;labels=suggestion&amp;body=\" + &lt;QUERY_FOR_ISSUE&gt; %}\n</code></pre> <p>{% endraw %}</p> <p>As\u00ed se ve en <code>query.html</code></p> <pre><code>... {% set query_for_issue = \"%23+title%0A%0A%23+query%0A%60%60%60sql%0A\" +\nrequest.query_string.split('sql=')[-1] + \"%0A%60%60%60\" %} {% set\nlink_to_new_issue =\n\"https://GitHub.com/chekos/ripa-2018-datasette/issues/new?title=Query+suggestion&amp;labels=suggestion&amp;body=\"\n+ query_for_issue %}\n\n&lt;p&gt;\n  &lt;button id=\"sql-format\" type=\"button\" hidden&gt;Format SQL&lt;/button&gt;\n  &lt;input type=\"submit\" value=\"Run SQL\" /&gt;\n  &lt;button type=\"button\" class=\"btn btn-secondary btn-sm\"&gt;\n    &lt;a href=\"{{ link_to_new_issue }}\" target=\"_blank\"\n      &gt;Submit on &lt;i class=\"fab fa-github\"&gt;&lt;/i\n    &gt;&lt;/a&gt;\n  &lt;/button&gt;\n&lt;/p&gt;\n...\n</code></pre> <p>{% endraw %}</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#cuanto-tiempo-tomo","title":"cuanto tiempo tom\u00f3","text":"<p>Todo el proceso tom\u00f3 alrededor de 20 horas en total, distribuidas en 3 a 4 semanas. La mayor parte estaba planeando y orquestando todo el proceso automatizado usando GitHub Actions. Espero que este tutorial te ayude a ahorrar esas horas extra. Al igual que cualquier otro proyecto de datos, es mejor pasar un tiempo por adelantado pensando y planeando detalladamente cada paso del proceso.</p>","tags":["sql","datasette","python"]},{"location":"2020/06/haciendo-datos-abiertos-m%C3%A1s-accesibles-con-datasette/#llamado-a-la-accion","title":"llamado a la acci\u00f3n","text":"<p>El mundo ha cambiado. La idea de este proyecto se me ocurri\u00f3 a mediados de abril y no cre\u00e9 el repositorio de GitHub hasta finales de mes. No comenc\u00e9 a escribir este tutorial hasta la segunda mitad de mayo. Todo ha cambiado desde entonces.</p> <p>Todos los d\u00edas en las redes sociales vemos videos y leemos historias sobre autoridades que abusan de su poder y ejercen violencia contra personas inocentes. No son todos, pero no deber\u00eda ser ninguno de ellos.</p> <p>Creo en los datos como una herramienta para el cambio, para la rendici\u00f3n de cuentas, para la transparencia. El mundo ha cambiado para siempre por la tecnolog\u00eda y los datos. Como tecn\u00f3logos, nerds de datos, expertos en pol\u00edticas o cualquier etiqueta autoascrita que elijan, tu que me lees, tenemos la responsabilidad de que nuestros pares usen nuestras habilidades, nuestras poderosas herramientas, nuestro conocimiento para hacer del mundo un lugar mejor porque podemos.</p> <p>Te pido que pienses en el papel que desempe\u00f1as. \u00bfHar\u00e1s del mundo un lugar mejor con las herramientas que est\u00e1s construyendo, por favor?</p> <p>Te dejo con el siguiente pensamiento de Data4BlackLives:</p> <p>Data for Black Lives es un movimiento de activistas, organizadores y matem\u00e1ticos comprometidos con la misi\u00f3n de utilizar la ciencia de datos para crear un cambio concreto y medible en la vida de las personas negras. Desde el advenimiento de la inform\u00e1tica, la big data y los algoritmos han penetrado pr\u00e1cticamente todos los aspectos de nuestra vida social y econ\u00f3mica. Estos nuevos sistemas de datos tienen un enorme potencial para empoderar a las comunidades de color. Herramientas como el modelado estad\u00edstico, la visualizaci\u00f3n de datos y el abastecimiento p\u00fablico, en las manos correctas, son instrumentos poderosos para combatir el sesgo, construir movimientos progresivos y promover el compromiso c\u00edvico. Pero la historia cuenta una historia diferente, una en la que los datos se utilizan con demasiada frecuencia como un instrumento de opresi\u00f3n, que refuerza la desigualdad y perpet\u00faa la injusticia. Redlining fue una empresa basada en datos que result\u00f3 en la exclusi\u00f3n sistem\u00e1tica de las comunidades negras de los servicios financieros clave. Las tendencias m\u00e1s recientes como la vigilancia predictiva, las sentencias basadas en el riesgo y los pr\u00e9stamos abusivos son variaciones preocupantes sobre el mismo tema. Hoy, la discriminaci\u00f3n es una empresa de alta tecnolog\u00eda.  - d4bl.org</p> <p>Este art\u00edculo fue publicado originalmente en ingl\u00e9s en la publicaci\u00f3n Towards Data Science, Making open data more accessible with datasette</p> <p>Share on  Share on  Share on </p>","tags":["sql","datasette","python"]},{"location":"2020/02/ya-se-fueron-las-nieves-de-enero/","title":"Ya se fueron las nieves de enero","text":"<p>Cuando todo va mal es cuando m\u00e1s me motivo / Respiro mejor en \u00e9ste ambiente nocivo / Esto para el rap es lo m\u00e1s nutritivo / Es todo lo que soy por eso escribo. - Nocivos, Faruz Feet &amp; Proof</p>"},{"location":"2020/02/ya-se-fueron-las-nieves-de-enero/#por-que-haria-otro-blog","title":"\u00bfPor qu\u00e9 har\u00eda otro blog?","text":"<p>Si me sigues en las redes sociales tal vez conozcas de la comunidad tacosdedatos que comenc\u00e9 a inicios del 2019. La idea de tacosdedatos es crear contenido en espa\u00f1ol. Contenido sobre el an\u00e1lisis y la visualizaci\u00f3n de datos. Contenido sobre mejores pr\u00e1cticas. Contenido sobre t\u00e9cnicas y tendencias en el mundo de la tecnolog\u00eda que parecen quedarse en el mundo angloparlante por mucho tiempo antes de llegar al mundo hispanohablante. </p> <p>tacosdedatos ha crecido mucho en este \u00faltimo a\u00f1o. Tenemos m\u00e1s de 3,400 seguidores en Twitter, miles de visitas al sitio web y lo m\u00e1s importante: muchas personas participando en la conversaci\u00f3n del an\u00e1lisis / visualizaci\u00f3n de datos en espa\u00f1ol. Cada semana recibo por lo menos un mensaje directo en twitter o un correo con dudas de c\u00f3mo comenzar en Python o R o como resolver X problema con su c\u00f3digo o como crear X visualizaci\u00f3n.  Tal vez a muchas personas no les impresione hablar con un extra\u00f1o pidiendo ayuda pero eso es lo que me llena de ganas de seguir haciendo lo que hago. </p> <p>Entonces, si a tacosdedatos le va tan bien, \u00bfpor qu\u00e9 crear otro blog? La verdad es que aunque existen ya muchos \"productos\" - el bolet\u00edn, el canal de Youtube, la academia, los dos podcast, el sitio web - todav\u00eda existen muchas cosas de las que me gustar\u00eda escribir y simplemente no \"caben\" en la filosof\u00eda de tacosdedatos. Por ejemplo, yo antes de crear tacosdedatos hab\u00eda creado elblogdehiphop d\u00f3nde hac\u00eda an\u00e1lisis (si eso le podemos llamar a lo que hac\u00eda jaja) y visualizaciones de datos sobre el Hip Hop latino-americano. Si siguen a Noisey en Espa\u00f1ol tal vez habr\u00e1n visto esta nota d\u00f3nde \"aporte\" una humilde opini\u00f3n de las mejores canciones de 2017: Las mejores canciones del rap mexicano en 2017. </p> <p>Este tipo de proyectos no son did\u00e1cticos por naturaleza; algo que me gustar\u00eda mantener en todo relacionado a tacosdedatos. </p> <p>La segunda raz\u00f3n es que si yo escribiera todas estas ideas en tacosdedatos.com saturar\u00eda el sitio y la idea es crear una comunidad. Quiero que m\u00e1s personas escriban para tacosdedatos no que sea mi blog personal - es un foro. </p> <p>Y la tercera raz\u00f3n es que yo promuevo mucho el que todxs tengamos un sitio/blog personal para mostrar el trabajo que hacemos. Ya seas una dise\u00f1adora, una programadora, una analista - todxs necesitamos un \"portafolio\" que podamos presentar lo que hacemos de una manera pr\u00e1ctica y accesible. Este blog servir\u00e1 como ejemplo. </p>"},{"location":"2020/02/ya-se-fueron-las-nieves-de-enero/#como-lo-hice","title":"Como lo hice","text":"<p>La raz\u00f3n principal por la que decid\u00ed utilizar este m\u00e9todo de publicaci\u00f3n es la facilidad. Fast.ai creo un repositorio de GitHub que sirve como plantilla. Se encuentra aqu\u00ed. </p> <p>Puedes crear un repositorio apartir de el suyo que ya est\u00e1 listo para ser publicado en GitHub Pages de manera gratuita y f\u00e1cil. Lo \u00fanico que necesitas es una cuenta de GitHub y crear un repositorio llamado <code>&lt;USUARIO&gt;.github.io</code> d\u00f3nde USUARIO es tu nombre de usuario utilizando este enlace: https://GitHub.com/fastai/fast_template/generate/. Es decir, si creas una cuenta de GitHub con el usuario \"papichulo\" solo tienes que hacer clic en ese enlace y crear <code>papichulo.github.io</code> y ya. </p> <p>GitHub sabe que cuando creas un repositorio siguiendo esas convenciones es porque quieres usar GitHub Pages y el repositorio base incluye todos los archivos necesarios para configurar tu blog y publicarlo sin ning\u00fan problema. </p> <p>Esto es esencial porque como les mencion\u00e9 yo ya tengo muchos proyectos que debo mantener. Uno m\u00e1s, por m\u00e1s importante que sea, ser\u00eda costar\u00eda mucho trabajo mantener. </p> <p>Usando este m\u00e9todo me tard\u00e9 3 minutos en crear este blog y para agregar contenido solo toma agregar archivos <code>.md</code> a la carpeta <code>_posts/</code>. </p> <p>De hecho, este post lo escrib\u00ed en mi tel\u00e9fono en camino a casa. Esa es la facilidad que necesito para tener otro blog. </p> <p>Esa es la facilidad que t\u00fa tienes si decides comenzar uno tambi\u00e9n. </p>"},{"location":"2020/02/ya-se-fueron-las-nieves-de-enero/#tonx-que-de-que-o-que","title":"tonx que de que o qu\u00e9","text":"<p>\u00bfSobre qu\u00e9 voy a escribir en este blog? Honestamente, no lo s\u00e9. Probablemente ser\u00e1 un lugar para presumir alguna visualizaci\u00f3n que haya hecho, o un an\u00e1lisis de mis versos de rap mexicano favoritos (\u00bfhaz escuchado Verbal Big Bang del Anexo Leiruk? \u00bfSuelo so\u00f1ar, correr y tropezar de Gera MX? \u00bfUnorthodox (DJ kingklan remix) de Eptos y Buffon?) o tal vez sobre tendencias en el mundo de la tecnolog\u00eda o cosas que me enojen del mundo (como lo que est\u00e1 sucediendo con el libro American Dirt en estados unidos). </p> <p>Lo m\u00e1s probable es que haga peque\u00f1os blogs sobre c\u00f3mo logr\u00e9 hacer algo en Python o R o d3. Micro tutoriales.</p> <p>Necesitamos m\u00e1s de eso en espa\u00f1ol. Existe stack overflow y existen blogs en ingl\u00e9s pero para los que apenas vamos comenzando en ciertas cosas el tener que aprender algo nuevo y aprenderlo en ingl\u00e9s es un reto m\u00e1s imponente de lo que deber\u00eda ser. </p> <p>En el anuncio de <code>fast_template</code> comparten la liga a un blog que la cofundadora de fast.ai escribi\u00f3 sobre bloguear. En \u00e9l menciona, entre otras cosas, que la mejor persona para ense\u00f1arle a alguien que est\u00e1 un paso detr\u00e1s c\u00f3mo hacer algo eres t\u00fa. Es decir, si acabas de aprender c\u00f3mo funcionan las <code>geom</code>s en <code>ggplot2</code> o como aplicar las etiquetas de los datos a un dataframe de <code>pandas</code> - t\u00fa eres la mejor persona para ense\u00f1arle c\u00f3mo hacerlo a alguien que est\u00e1 justo queriendo aprender a hacer eso. Por eso, no importa tu nivel de experiencia, t\u00fa deber\u00edas tener un blog. </p> <p>Te recomiendo <code>fast_template</code> c\u00f3mo lo estoy haciendo yo aqu\u00ed pero tambi\u00e9n existe Medium.com y hasta tacosdedatos.com. Lo importante es comenzar. </p>"},{"location":"2020/02/ya-se-fueron-las-nieves-de-enero/#conclusiones","title":"conclusiones","text":"<p>Todxs tenemos algo que decir. </p> <p>Tal vez pienses que como principiante no hay raz\u00f3n por qu\u00e9 tener un blog - yo opino lo contrario. Necesitamos m\u00e1s contenido nivel principiante e intermedio en espa\u00f1ol. </p> <p>Tal vez pienses que nadie lo va a leer. Te prometo que m\u00e1s personas de las que crees est\u00e1n esperando lo que vas a escribir. </p> <p>Tal vez pienses es muy dif\u00edcil. Si usas la plantilla <code>fast_template</code> est\u00e1s a unos cuantos clics de tener tu blog - no necesitas saber Git no GitHub ni HTML ni javascript ni nada. </p> <p>Comienza tu blog.</p> <p>Si necesitas ayuda m\u00e1ndame un mensaje por twitter, Instagram, Facebook o hasta un correo \ud83d\udce8.</p> <p>Share on  Share on  Share on </p>"},{"location":"2025/07/of-brasas--nube/","title":"Of Brasas &amp; Nube","text":"<p>Translation Note</p> <p>This is an English translation of a piece originally published in Spanish on the Tacos de Datos Substack. The original Spanish version explored these themes for a Spanish-speaking audience interested in data and technology.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#part-1-6-weeks-6-projects-4-products","title":"part 1: 6 weeks, 6 projects, 4 products","text":"","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#stolen-time","title":"Stolen Time","text":"<p>In six weeks, I stole 350 hours from a future that never existed. (Is stealing time a federal crime? Don't deport me.)</p> <p>While I cooked dinner and watched Netflix, AI agents were building in a parallel reality. They executed my ideas at an accelerated pace, generating work from which I reaped the rewards. The result: four working products that would have taken months of traditional development.</p> <p>This parallel reality produced:</p> <ul> <li>121 features shipped</li> <li>4 live products launched</li> <li>22 critical fixes deployed</li> </ul> <p>All while I lived my normal life. No late nights. No missed movies. No sacrificed weekends.</p> <p>But here's the tension: how do we value work that no one did? What happens to an economy where time itself becomes elastic? When anyone can build in parallel time, what makes anything valuable?</p> <p>This is the story of how I built a constellation of tools without sacrificing a single minute of my real life\u2014and what this tells us about the future of human work.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#the-constelation","title":"The Constelation","text":"","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#amoxcalli-natural-memory-amplified-57-tickets","title":"Amoxcalli: Natural Memory, Amplified (57 tickets)","text":"<p>Your brain never stops weaving connections. A morning traffic pattern sparks a systems theory insight. \"Loco, no soy el m\u00e1s bueno, yo soy el mas aferrado\" suddenly illuminates your role in the AI revolution. These threads of thought, invisible but constant, are what make us human.</p> <p>But they're also fragile. Each connection exists for a moment, then fades\u2014lost to time, to sleep, to the next thought demanding attention. We accept this as the natural limitation of consciousness.</p> <p>Amoxcalli emerged from this tension\u2014not to replace these fleeting moments, but to give them permanence. It's like having a shadow self that never sleeps, never forgets, never stops connecting:</p> <ul> <li>While you drive, your voice captures an idea.</li> <li>While you sleep, the system weaves connections.</li> <li>While you work, patterns emerge naturally.</li> <li>While you live, your network of thoughts grows.</li> </ul> <p>But what happens when your thoughts outlive their moments? When connections keep growing without you?</p> <p>The technology is simple: voice capture, semantic networks, invisible interfaces. But the implications are dizzying: a parallel mind that builds while you rest, connects while you forget, grows while you live.</p> <p>It's not artificial memory\u2014it's natural memory freed from time. But maybe time was the point all along. Maybe the beauty of human thought lies precisely in its impermanence, in the way each connection must be rediscovered, each pattern seen anew.</p> <p>Or maybe that's just what we tell ourselves because we never had a choice before.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#fieldnotes-designing-moments-of-wonder","title":"Fieldnotes: Designing Moments of Wonder","text":"<p>There's a moment every teacher knows: when a student's eyes light up with understanding, when confusion transforms into clarity, when a lesson transcends its content and becomes an experience. These moments are pure alchemy\u2014part preparation, part intuition, part inexplicable magic.</p> <p>But they're ephemeral. You can plan for them, hope for them, but you can't manufacture wonder. At least, that's what we believed.</p> <p>Fieldnotes began as an experiment: could we map the DNA of wonder? Could we analyze thousands of magical teaching moments and extract their patterns? The result is unsettling in its effectiveness:</p> <ul> <li>You describe a feeling you want to evoke, and algorithms suggest proven paths to wonder.</li> <li>You sketch an idea, and AI personas offer variations, each targeting different learning styles.</li> <li>You build a lesson, and machine learning predicts emotional resonance, suggests adjustments.</li> </ul> <p>It works. Maybe too well.</p> <p>What does it mean when inspiration becomes predictable? When we can manufacture moments that feel spontaneous, engineer experiences that feel organic? Some teachers embrace it\u2014finally, a way to reliably create those transformative moments. Others resist\u2014arguing that true wonder can't be reduced to patterns and predictions.</p> <p>Both are right, probably. But we're past the point of choosing. The question isn't whether to use these tools, but how to use them while preserving what makes teaching human. How to harness pattern recognition without becoming slaves to the pattern.</p> <p>The technology is just code interpreting data. But in education, data is human experience, pattern is emotion, and optimization means shaping how people learn to see the world.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#alnilam-the-star-that-guides","title":"Alnilam: The Star That Guides","text":"<p>We still navigate careers like ancient sailors\u2014by fixed stars, steady paths, predictable horizons. Three years of experience. Five years to senior. A decade to director. These markers made sense when time moved at human speed.</p> <p>But what happens when time becomes elastic? When you can compress a year's worth of iterations into a week? When your commits span parallel timelines, and your changelog reads like science fiction?</p> <p>Alnilam emerged from this paradox. Living in the terminal\u2014where time is measured in commits and progress in pull requests\u2014it watches as traditional career paths collide with parallel development:</p> <ul> <li>Your git log shows three months of work done overnight</li> <li>Your project history spans multiple temporal threads</li> <li>Your experience grows in parallel dimensions</li> </ul> <p>The data tells an impossible story: you're simultaneously a junior dev and a system architect. A beginner and a veteran. Present and future tangled in your commit history.</p> <p>The old metrics break down. What's a \"year of experience\" when you can run a thousand experiments in parallel? What's \"senior level\" when your AI agents are shipping features while you sleep? </p> <p>Yet somehow the stars still matter. Maybe more than ever. Because in this elastic time, we need fixed points. Not to follow blindly, but to question our trajectory: Are we actually growing, or just moving faster? Are we becoming better developers, or just more efficient producers?</p> <p>Alnilam doesn't answer these questions. It sits in your terminal, quietly logging as you bend time, offering data but no judgments. A reminder that even in parallel timelines, even with AI acceleration, the hardest navigation is still internal\u2014knowing not just where you're going, but why.</p> <p>It's not artificial career planning\u2014it's a compass for elastic time.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#bns-studio-honest-brutalism-joyful-design","title":"BNS Studio: Honest Brutalism, Joyful Design","text":"<p>In an era where AI can generate infinite complexity, we chose brutal simplicity. When algorithms can create any design, we stripped everything bare. BNS Studio began as a landing page and became a rebellion: what if we built something deliberately basic in a world of instant sophistication?</p> <p>The manifesto wrote itself:</p> <ul> <li>Show the scaffolding</li> <li>Celebrate the structure</li> <li>Let the machine be a machine</li> <li>Make buttons look like buttons</li> <li>Give words room to breathe</li> <li>Hide nothing</li> <li>Load instantly or don't load at all</li> </ul> <p>It's almost funny\u2014using parallel time and AI acceleration to build something that looks like it could've been made in 1991. But that's the point. When you can generate anything, the most radical act is choosing to build almost nothing.</p> <p>The site loads instantly because it has almost nothing to load. It works offline because it barely needs the internet. It's accessible because there's nothing to make inaccessible. Every element exists because it must, not because it could.</p> <p>In a parallel reality of infinite design possibilities, we chose finite constraints. Not because we couldn't do more, but because we shouldn't. A reminder that even with godlike generative powers, beauty often lies in restraint.</p> <p>It's not anti-technology\u2014it's technology that's honest about what it is.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#project-rainfall-your-natural-study-space","title":"Project Rainfall: Your Natural Study Space","text":"<p>We named it Rainfall because that's how knowledge should feel: natural, nourishing, inevitable. Not a storm that shows off its power, but a steady rain that helps things grow.</p> <p>But first it was \"AI Study Workbench\" (very San Francisco, very $8 coffee). Then \"Brainstorm\" (peak Silicon Valley). The names evolved as we understood what we were actually building: not another AI tool, but a space that preserves the natural rhythm of thought.</p> <p>It started with a simple observation: thinking happens in the gaps between apps. In the moments between capturing and connecting, between reading and understanding. Every context switch is a moment where insight might escape.</p> <p>Rainfall closes these gaps:</p> <ul> <li>Your keystrokes capture without breaking flow (\u2318\u21e7C for vision, \u2325Space for voice)</li> <li>AI runs locally, processing privately, respecting your machine's power</li> <li>Your thoughts stay whole, uninterrupted by cloud sync or connection drops</li> </ul> <p>The technology disappears. Not because it's hidden, but because it fits the natural shape of thought. Like rain falling exactly where it's needed.</p> <p>It's our most honest project yet. No parallel processing. No time manipulation. Just technology shaped to human rhythm. A reminder that even in this age of infinite acceleration, some things\u2014like understanding, like insight, like wisdom\u2014still need their natural time.</p> <p>Or maybe that's just what we tell ourselves as we build tools that bend reality. Maybe we're all just trying to make peace with these new powers, finding ways to make them feel as natural as rain.?</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#what-i-discovered","title":"What I Discovered","text":"","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#the-paradox-of-parallel-time","title":"The Paradox of Parallel Time","text":"<p>We thought AI would automate our work. Instead, it created a parallel dimension where work happens without us. Not replacing human effort, but existing alongside it\u2014a shadow reality where our intentions execute while we sleep, while we rest, while we live.</p> <p>The 350 hours that built these tools never existed in my timeline. They were borrowed from a future that won't happen, executed in a present I never experienced. What does it mean to create value in time that technically never was?</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#the-tension-of-natural-artifice","title":"The Tension of Natural Artifice","text":"<p>Each project began with the same goal: extend what humans naturally do. But extension became transformation:</p> <ul> <li>Memory beyond biological limits</li> <li>Wonder reduced to algorithms</li> <li>Experience compressed into parallel threads</li> <li>Design stripped to brutal honesty</li> <li>Thought flowing like rain</li> </ul> <p>We aimed to amplify human capability. We ended up questioning what \"human\" means.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#the-death-of-scarcity","title":"The Death of Scarcity","text":"<p>Time was always our scarcest resource. Every hour spent coding was an hour not living. Every minute debugging was a minute not creating. Or at least, that's how it used to work.</p> <p>But when work can happen in parallel time, when development can occur in dimensions we never experience, scarcity dissolves. And with it goes everything we built on that foundation\u2014our notions of value, of effort, of worth.</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#the-questions-that-remain","title":"The Questions That Remain","text":"<p>The technical problems were the easy part. The hard questions are the ones we're still afraid to ask:</p> <p>What happens to human creativity when every idea can be instantly manifested? What's the value of expertise when time becomes elastic? How do we measure growth when experience can be parallelized? What remains fundamentally human when every natural limit can be transcended?</p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2025/07/of-brasas--nube/#a-final-question","title":"A Final Question","text":"<p>I stole 350 hours from a future that will never exist. But maybe that's not quite right.</p> <p>Maybe I didn't steal time\u2014maybe I discovered that time was never what we thought it was. That our neat divisions between past and future, between work and life, between human and machine, were always more arbitrary than we admitted.</p> <p>Or maybe that's just what we tell ourselves as we learn to wield powers we don't fully understand, building tools that feel like rain but move like lightning.</p> <p>Either way, there's no going back. The only question is: as we bend reality to our will, which parts of our humanity are we willing to let bend with it?</p> <p>Share on  Share on  Share on </p>","tags":["parallel-time","development","ai-tools","productivity"]},{"location":"2020/06/escaping-liquid-tags-with--raw-/","title":"about escaping liquid tags with <code>{% Raw %}</code>","text":"","tags":["jekyll","liquid tags"]},{"location":"2020/06/escaping-liquid-tags-with--raw-/#what-i-learned","title":"what i learned","text":"<p>you can use the tags <code>raw</code> and <code>endraw</code> to escape liquid tags.</p>","tags":["jekyll","liquid tags"]},{"location":"2020/06/escaping-liquid-tags-with--raw-/#how-i-learned","title":"how i learned","text":"<p>in the previous TIL i tried to write the following</p> <p>{% raw %} i am moving soyserg.io from a hugo site to jekyll and i couldn't figure out how to have nested <code>{{}}</code> as in <code>{{ category/{{category | downcase }} | relative_url }}</code> which just doesn't work. so i moved to <code>{{ 'category/' | append: category | downcase | relative_url }}</code></p> <p>but it was not possible off the bat because of the double <code>{}</code>. at first i tried to escape them as <code>\\{\\{\\}\\}</code> but that did not work.</p> <p>{% raw %} turns out the answer is the <code>{% raw %}</code> tag.</p> <p>the paragraph then looks like this in the markdown file</p> <p>{% raw %}</p> <pre><code>`{% raw %}`\ni am moving soyserg.io from a hugo site to jekyll and i couldn't figure out how to have nested `{{}}` as in `{{ category/{{category | downcase }} | relative_url }}` which just doesn't work. so i moved to `{{ 'category/' | append: category | downcase | relative_url }}`\n`% endraw %`\n</code></pre> <p>{% endraw %}</p>","tags":["jekyll","liquid tags"]},{"location":"2020/06/escaping-liquid-tags-with--raw-/#visit","title":"visit","text":"<p>https://stackoverflow.com/questions/24102498/escaping-double-curly-braces-inside-a-markdown-code-block-in-jekyll</p> <p>Share on  Share on  Share on </p>","tags":["jekyll","liquid tags"]},{"location":"2020/05/about-jekyll-archives/","title":"about <code>jekyll-archives</code>","text":"","tags":["jekyll"]},{"location":"2020/05/about-jekyll-archives/#what-i-learned","title":"what i learned","text":"<p>there's a jekyll-plugin named <code>jekyll-archives</code> that lets you create archives of your posts based on metadata/front matter.</p> <p>i used it for socialtech.us to create a <code>category/${category}</code> page for each of the categories in the site (created dynamically).</p>","tags":["jekyll"]},{"location":"2020/05/about-jekyll-archives/#how-i-learned","title":"how i learned","text":"<p>the social tech collaborative website has a <code>/categories/</code> page where each category has a header so you can get to each category via url like <code>socialtech.us/categories#${category}</code>. for example, you could go to <code>socialtech.us/categories#texting</code> if you wanted to see the plays with the tag <code>texting</code>. however, because each play can have multiple categories, plays would appear multiple times in the <code>/categories</code> page. we wanted a page per category.</p> <p>at first i thought i was going to have to some wild logic in a page to \"fill\" in each category and maybe use the page's query params but that would not be done through jekyll since jekyll creates the static website only - you can't use jekyll or liquid tags to play around with the query params.</p> <p>the answer was found here: github.com/jekyll/jekyll/issues/5672</p> <p>all that was needed was</p> <ol> <li>to add <code>jekyll-archives</code> to the list of plug-ins 2.</li> </ol> <pre><code># Archives\njekyll-archives:\n  enabled: ['categories']\n  layout: archive\n  permalinks:\n    category: '/category/:name/'\n</code></pre> <ol> <li>add an <code>archive.html</code> to <code>_layouts/</code></li> </ol>","tags":["jekyll"]},{"location":"2020/05/about-jekyll-archives/#visit","title":"visit","text":"<p>https://github.com/jekyll/jekyll-archives</p> <p>Share on  Share on  Share on </p>","tags":["jekyll"]},{"location":"2022/01/jq--syntax/","title":"about jq <code>[ ]</code> syntax","text":"","tags":["jq","shell"]},{"location":"2022/01/jq--syntax/#what-i-learned","title":"what i learned","text":"<p>If you want to dump a list of objects you\u2019re constructing from some other json you need to wrap your entire <code>jq</code> string in square brackets ( <code>[]</code> ). Otherwise you\u2019ll be writing each object one at a time and that\u2019s not valid JSON. For example, running something like</p> <pre><code>jq '.[] | {id: .id, title: .title, created: .created }'\n</code></pre> <p>returns \u2192</p> <pre><code>{\n    id: \"123\",\n    title: \"page 1\",\n    created: \"2022-01-25T23:15:00.000Z\"\n}\n{\n    id: \"124\",\n    title: \"page 2\",\n    created: \"2022-01-26T13:18:15.000Z\"\n}\n{\n    id: \"125\",\n    title: \"page 3\",\n    created: \"2022-01-27T18:37:05.000Z\"\n}\n</code></pre> <p>This file is not valid JSON. However, if you wrap your entire expression in square brackets <code>[]</code> <code>jq</code> will group these all as a list of objects instead of appending each object at a time.</p> <pre><code>jq '[.[] | { id: .id, title: .title, created: .created }]'\n</code></pre> <p>returns \u2192</p> <pre><code>[\n  {\n    \"id\": \"123\",\n    \"title\": \"page 1\",\n    \"created\": \"2022-01-25T23:15:00.000Z\"\n  },\n  {\n    \"id\": \"124\",\n    \"title\": \"page 2\",\n    \"created\": \"2022-01-26T13:18:15.000Z\"\n  },\n  {\n    \"id\": \"125\",\n    \"title\": \"page 3\",\n    \"created\": \"2022-01-27T18:37:05.000Z\"\n  }\n]\n</code></pre>","tags":["jq","shell"]},{"location":"2022/01/jq--syntax/#how-i-learned","title":"how i learned","text":"<p>Testing the <code>til-notion-integration</code> and <code>markdownify-notion</code> I tried reading a list of TILs I had saved in a JSON file. However, each object was separated by a new line - not a comma.</p>","tags":["jq","shell"]},{"location":"2022/01/jq--syntax/#reference","title":"reference","text":"<p>The solution (after many failed google searches) was found on a GitHub issue answered by the creator of <code>jq</code> \u2192 github.com/stedolan/jq/issues/124</p> <p>Share on  Share on  Share on </p>","tags":["jq","shell"]},{"location":"2020/06/about-liquid-tags/","title":"about liquid tags","text":"","tags":["jekyll","liquid tags"]},{"location":"2020/06/about-liquid-tags/#what-i-learned","title":"what i learned","text":"<p>shopify has a github pages site documenting liquid tags which is very useful and easy to use.</p>","tags":["jekyll","liquid tags"]},{"location":"2020/06/about-liquid-tags/#how-i-learned","title":"how i learned","text":"<p>{% raw %} i am moving soyserg.io from a hugo site to jekyll and i couldn't figure out how to have nested <code>{{}}</code> as in <code>{{ category/{{category | downcase }} | relative_url }}</code> which just doesn't work. so i moved to <code>{{ 'category/' | append: category | downcase | relative_url }}</code></p> <p>the answer was found here: shopify.github.io/liquid</p> <p>Share on  Share on  Share on </p>","tags":["jekyll","liquid tags"]},{"location":"2020/06/about-myst-parser-es-markdown-pero-rst/","title":"about <code>myst-parser</code>, es markdown pero rst","text":"","tags":["sphinx","docs"]},{"location":"2020/06/about-myst-parser-es-markdown-pero-rst/#what-i-learned","title":"what i learned","text":"<p>MyST-parser is so easy to use and it provides all the functionality i need to make docs without using rST.</p>","tags":["sphinx","docs"]},{"location":"2020/06/about-myst-parser-es-markdown-pero-rst/#how-i-learned","title":"how i learned","text":"<p>i was creating the docs for <code>tacosdedatos-utils</code> and i rather not touch rST ever. i had seen @choldraf tweet about MyST before but i had not had the need to use it.</p> <p>today i made the documentation for that small utils package and it was actually a pleasant experience? the underlying concepts of rST (roles, directives, etc etc) are still a little fuzzy but the syntax makes a ton more sense now so it's one less challenge for me.</p> <p>official docs: myst-parser.readthedocs.io</p> <p>Share on  Share on  Share on </p>","tags":["sphinx","docs"]},{"location":"2020/05/you-cant-use-special-characters-in-unix-commands-if-you-use-single-quotes/","title":"about special characters in unix commands","text":"","tags":["unix"]},{"location":"2020/05/you-cant-use-special-characters-in-unix-commands-if-you-use-single-quotes/#what-i-learned","title":"what i learned","text":"<p>You can't insert variables in Unix commands if you're using single quotes.</p> <p>this won't work</p> <pre><code>export SECRET=\"huh\"\n\necho 'you can not see my secret $SECRET'\n</code></pre> <p>but this will</p> <pre><code>export SECRET=\"huh\"\n\necho \"you can see my secret $SECRET\"\n</code></pre>","tags":["unix"]},{"location":"2020/05/you-cant-use-special-characters-in-unix-commands-if-you-use-single-quotes/#how-i-learned","title":"how i learned","text":"<p>while setting up a GitHub action for the social tech collaborative website that would send a url to a specific slack channel, i would get <code>$TARGET_URL</code> instead of the actual url.</p> <p>turns out special characters are interpreted as literals with single-quotes.</p> <p>when you use double-quotes, special characters <code>$</code>, <code>\\</code> and <code>`</code> remain special \ud83d\ude44</p> <p>Single quotes (' ') operate similarly to double quotes, but do not permit referencing variables, since the special meaning of $ is turned off. Within single quotes, every special character except ' gets interpreted literally. Consider single quotes (\"full quoting\") to be a stricter method of quoting than double quotes (\"partial quoting\").  - tldp.org</p> <p>Share on  Share on  Share on </p>","tags":["unix"]},{"location":"2024/11/creating-til-posts-from-github-issues-using-github-actions/","title":"creating til posts from github issues using github actions","text":"","tags":[]},{"location":"2024/11/creating-til-posts-from-github-issues-using-github-actions/#what-i-learned","title":"what i learned","text":"<p>you can automate creating a new markdown file in a directory in your repo with front matter metadata from github issues. you can then create a pull request to deploy those changes to your main branch. my plan is to use this to capture more ideas on the go (on my phone).</p> .github/workflows/issue-to-md.yml<pre><code>name: Create Post from Issue\n\npermissions:\n  contents: write\n  pull-requests: write\n\non:\n  issues:\n    types: [opened]\n\njobs:\n  create-post:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Generate Post from Issue\n        env:\n          ISSUE_NUMBER: ${{ github.event.issue.number }}\n          ISSUE_TITLE: ${{ github.event.issue.title }}\n          ISSUE_BODY: ${{ github.event.issue.body }}\n          ISSUE_LABELS: ${{ toJson(github.event.issue.labels) }}\n          ISSUE_CREATED_AT: ${{ github.event.issue.created_at }}\n        run: |\n          # Convert labels to a list of tags\n          TAGS=$(echo $ISSUE_LABELS | jq -r '.[] | .name' | paste -sd, -)\n\n          # Convert ISSUE_CREATED_AT to PST and format as YYYY-MM-DD\n          CREATED_AT_PST=$(TZ=\"America/Los_Angeles\" date -d \"${ISSUE_CREATED_AT}\" +\"%Y-%m-%d\")\n\n          # Extract the category from the part of the title before the first colon, default to \"project\" if none\n          CATEGORY=$(echo \"$ISSUE_TITLE\" | awk -F: '{print $1}' | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')\n          if [ -z \"$CATEGORY\" ]; then\n            CATEGORY=\"project\"\n          fi\n\n          # Extract the title content after the first colon\n          TITLE=$(echo \"$ISSUE_TITLE\" | sed 's/^[^:]*: *//')\n\n          # Determine directory based on category\n          if [ \"$CATEGORY\" = \"til\" ]; then\n            DIR=\"blog/posts/til\"\n          else\n            DIR=\"blog/posts\"\n          fi\n          echo $DIR &gt;&gt; $GITHUB_STEP_SUMMARY\n          echo $CATEGORY &gt;&gt; $GITHUB_STEP_SUMMARY\n\n          # Generate a slugified version of the title for the filename\n          SLUG=$(echo \"$TITLE\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '-' | sed 's/^-//;s/-$//')\n\n          echo $SLUG &gt;&gt; $GITHUB_STEP_SUMMARY\n\n          # Create the front matter with category, tags, and formatted date\n          FRONT_MATTER=\"---\\ntitle: \\\"$TITLE\\\"\\ndate: ${CREATED_AT_PST}\\ncategories: [${CATEGORY}]\\ntags: [${TAGS}]\\n---\"\n\n          # Prepare content for markdown file\n          CONTENT=\"$FRONT_MATTER\\n\\n$ISSUE_BODY\"\n\n          # Save the content to a markdown file\n          FILENAME=\"${DIR}/${SLUG}.md\"\n          echo $FILENAME &gt;&gt; $GITHUB_STEP_SUMMARY\n          echo -e \"$CONTENT\" &gt; \"$FILENAME\"\n\n      - name: Commit and push changes\n        env: \n          ISSUE_TITLE: ${{ github.event.issue.title }}\n          ISSUE_NUMBER: ${{ github.event.issue.number }}\n          GH_TOKEN: ${{ github.token }}\n        run: |\n          git config --local user.name \"github-actions[bot]\"\n          git config --local user.email \"github-actions[bot]@users.noreply.github.com\"\n          git checkout -b add-post-$ISSUE_NUMBER\n          git add .\n          git commit -m \"Add post for Issue: $ISSUE_TITLE\"\n          git push -u origin add-post-$ISSUE_NUMBER\n          gh pr create --title \"#$ISSUE_NUMBER - $ISSUE_TITLE\" --body \"Adding new post. Closes #$ISSUE_NUMBER\"\n</code></pre>","tags":[]},{"location":"2024/11/creating-til-posts-from-github-issues-using-github-actions/#how-i-learned","title":"how i learned","text":"<p>i was trying to set up a new blog using <code>quarto</code> and i wanted a way to minimize friction to write more. i try to keep my _til_s pretty standard with three sections: what i learned, how i learned, and reference. because they also live in a blog published in github pages using (currently using mkdocs-material blog plugin) they all have to have the <code>categories</code> front matter metadata <code>til</code>. so, it\u2019s really templated. i just need an easy way to write something quick in markdown and put it in the repo but going to the repo\u2019s url, clicking new file, naming it, adding front matter by hand, etc. is just not great. I also don\u2019t want to break git history if I had already started something locally on my laptop so i needed a way to add a new post in a new PR. this also gives me the opportunity to edit further if needed. </p> <p>i asked chatgpt for the github action yaml with those specifications and iterated a bit. now, i use ulysses on my iphone to write a post and just export to markdown, copy and paste it to a new issue. </p>","tags":[]},{"location":"2024/11/creating-til-posts-from-github-issues-using-github-actions/#reference","title":"reference","text":"<ul> <li>chatgpt conversation: https://chatgpt.com/share/67341db2-fec8-8004-bbfa-70fb1cffe8d2</li> </ul> <p>Share on  Share on  Share on </p>","tags":[]},{"location":"2022/12/how-to-copy-json-straight-to-clipboard-from-the-terminal/","title":"how to copy json to clipboard from the terminal","text":"","tags":["jq","shell"]},{"location":"2022/12/how-to-copy-json-straight-to-clipboard-from-the-terminal/#what-i-learned","title":"what i learned","text":"<p>Piping output to <code>pbcopy</code> to copy and paste output from the terminal</p> <p>Here's how I can grab the last 10 elements of a JSON array and copy them to my clipboard.</p> <pre><code>jq '.[-10:]' mydata.json | pbcopy\n</code></pre>","tags":["jq","shell"]},{"location":"2022/12/how-to-copy-json-straight-to-clipboard-from-the-terminal/#how-i-learned","title":"how i learned","text":"<p>I've been working with my spotify streaming history data and it's a lot of nested data so I've been using <code>jq</code> a lot.</p> <p>I'm working on restructuring a complicated nested JSON so I went to  and I needed some sample data. I remember reading a tweet or maybe a tip on a Medium article about piping to <code>pbcopy</code> so I figured I'd try it and document it. <p>The final code looked more like</p> <pre><code>jq '.[-5:]' interim/streaming_history.json | pbcopy\n</code></pre>","tags":["jq","shell"]},{"location":"2022/12/how-to-copy-json-straight-to-clipboard-from-the-terminal/#reference","title":"reference","text":"<ul> <li><code>jq</code> play sandbox environment online:  <li>what i ended up using jqplay for: https://jqplay.org/s/AMl-8okc7or</li> <p>Share on  Share on  Share on </p>","tags":["jq","shell"]},{"location":"2022/12/how-to-create-an-alias-in-the-gh-cli/","title":"how to create an alias in the <code>gh</code> CLI","text":"","tags":["gh","automation"]},{"location":"2022/12/how-to-create-an-alias-in-the-gh-cli/#what-i-learned","title":"what i learned","text":"<p>you can create aliases in the GitHub CLI. i'm not super familiar with aliases. i've used them in the past to automate long commands. currently i'm using a couple at work to shorten <code>dbt</code> commmands ever so slightly (from <code>dbt run --target prod --select &lt;models&gt;</code> to <code>prod-run &lt;selection query&gt;</code>).</p> <p>however, i had only seen these as aliases one sets up at the profile level/scope. as in, we'd go to <code>~/.bash_profile</code> or <code>~/.zsh_profile</code> and add a new alias that's set every time we open a new terminal.</p> <p>this is the first time i see a cli offer that within the tool itself. i wonder if this is a common practice i've missed until now.</p> <p>in the GitHub cli you can use the command <code>alias set</code> to set an alias (docs).</p> <p>i usually have to google the full list of flags i would like to run when creating a repo via the <code>gh-cli</code> so i figured i'd save it as an alias now. this is why i ~~wish i remembered~~ would like to run most times:</p> <pre><code>gh repo create &lt;name&gt; \\\n--public \\\n--add-readme \\\n--clone \\\n--gitignore Python \\\n--license bsd-3-clause-clear\n</code></pre> <p>simply create a public repo named  include a ReadME, a license and a gitignore file and finally clone it to the local directory. <p>i might add the <code>--disable-wiki</code> simply because i don't use the wikis.</p> <p>from the docs:</p> <p>The expansion may specify additional arguments and flags. If the expansion includes positional placeholders such as \"$1\", extra arguments that follow the alias will be inserted appropriately. Otherwise, extra arguments will be appended to the expanded command.</p> <p>so what i did was run</p> <pre><code>gh alias set pyrepo 'repo create \"$1\" --public --add-readme --clone --gitignore=Python --license=bsd-3-clause-clear'\n</code></pre> <p>and if i choose to i can add a description by adding <code>-d \"my repos description\"</code> right after <code>gh pyrepo &lt;name&gt;</code></p>","tags":["gh","automation"]},{"location":"2022/12/how-to-create-an-alias-in-the-gh-cli/#how-i-learned","title":"how i learned","text":"<p>i've been creating lots of small project repos lately and this feels like a small automation that could solve some frustrations.</p>","tags":["gh","automation"]},{"location":"2022/12/how-to-create-an-alias-in-the-gh-cli/#reference","title":"reference","text":"<ul> <li>GitHub CLI manual: https://cli.github.com/manual/</li> <li><code>gh alias set</code></li> <li><code>gh repo create</code></li> <li>GitHub Docs about licenses: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository#searching-github-by-license-type</li> </ul> <p>Share on  Share on  Share on </p>","tags":["gh","automation"]},{"location":"2021/09/how-to-execute-a-shell-script-in-the-current-shell/","title":"how to execute a shell script in the current shell","text":"","tags":["shell"]},{"location":"2021/09/how-to-execute-a-shell-script-in-the-current-shell/#what-i-learned","title":"what i learned","text":"<p>when you execute a shell script, it defaults to creating a new shell, executing the script in that shell and closing it. if you want to, for example, set environmental variables you would need to run the script in the current shell. let's say you want to have a short shell script that sets the database url as an environmental variable called <code>env_vars.sh</code>.</p> <pre><code>#!/bin/bash\nexport DATABASE_URL=\"super_secret_url\"\n</code></pre> <p>if you run</p> <pre><code>sh env_vars.sh\n</code></pre> <p>in your terminal, it would run said script in a new shell and therefore those environmental variables would not be set in your current shell and would then be unavailable to your other scripts.</p> <p>to run that in your current shell you use the following syntax</p> <pre><code>. ./env_vars.sh\n</code></pre> <p>this way your environmental variables are set in your current shell and you can use them as expected.</p>","tags":["shell"]},{"location":"2021/09/how-to-execute-a-shell-script-in-the-current-shell/#how-i-learned","title":"how i learned","text":"<p>i'm testing <code>SQLModel</code> and wanted to test access to snowflake. instead of setting the environmental variables manually i thought i'd just run a script that had <code>export snowflake_username=\"xyz\"</code> etc, etc. however, when i ran the script and tried to use <code>os.environ['snowflake_username']</code> i'd get an error.</p> <p>finding the solution was surprisingly fast.</p>","tags":["shell"]},{"location":"2021/09/how-to-execute-a-shell-script-in-the-current-shell/#visit","title":"visit","text":"<p>https://stackoverflow.com/questions/496702/can-a-shell-script-set-environment-variables-of-the-calling-shell</p> <p>Share on  Share on  Share on </p>","tags":["shell"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/","title":"how to set up <code>ffmpeg</code> as a lambda layer","text":"","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#what-i-learned","title":"what i learned","text":"<p>how to add <code>ffmpeg</code> and <code>ffprobe</code> as a lambda layer to be used by lambda functions.</p>","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#getting-ffmpeg","title":"Getting ffmpeg","text":"<pre><code># ffmpeg\nwget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz\n\n# checksum\nwget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz.md5\n\nmd5sum -c ffmpeg-release-amd64-static.tar.xz.md5\n\n# extract\ntar xvf ffmpeg-release-amd64-static.tar.xz\n</code></pre> <p>Side note: i had to <code>brew install md5sha1sum</code> and <code>brew install wget</code> on my local laptop</p>","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#creating-lambda-layer","title":"Creating Lambda Layer","text":"<ol> <li>create <code>ffmpeg/bin/</code></li> <li>copy <code>ffmpeg</code> into it</li> <li>zip <code>ffmpeg/</code></li> </ol> <pre><code># Create bin/\nmkdir -p ffmpeg/bin\n\n# Copy ffmpeg\ncp ffmpeg-6.0-amd64-static/ffmpeg ffmpeg/bin\n\n# Zip directory\ncd ffmpeg\nzip -r ../ffmpeg.zip .\n</code></pre>","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#finally","title":"Finally","text":"<p>Upload zip file as a lambda layer.</p>","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#bonus","title":"Bonus","text":"<p>In my case I also included <code>ffprobe</code> as it's also required for <code>whisper</code>.</p>","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#how-i-learned","title":"how i learned","text":"<p>i've been using OpenAI's whisper and i want to set it up as a lambda function to transcribe files as they land on an S3 bucket.</p>","tags":["aws","serverless","ffmpeg"]},{"location":"2023/04/setting-up-ffmpeg-as-lambda-layer/#reference","title":"reference","text":"<ul> <li>AWS blog: \"Processing User Generated Content using AWS Lambda and FFmpeg</li> </ul> <p>Share on  Share on  Share on </p>","tags":["aws","serverless","ffmpeg"]},{"location":"2022/01/how-to-solve-permission-error-from-airflow-official-docker-image/","title":"how to solve permission error from airflow official docker image","text":"","tags":["docker","airflow","python"]},{"location":"2022/01/how-to-solve-permission-error-from-airflow-official-docker-image/#what-i-learned","title":"what i learned","text":"<p>tl;dr: when you use the Airflow official docker image you need to make sure that the variable <code>AIRFLOW_UID</code> is set to match your UID (and <code>AIRFLOW_GID=0</code> aka <code>root</code> ) or you\u2019re going to get permission errors. i was working on deploying Airflow on a VM at work this week and I got a permission error (Errno 13) regarding the containers\u2019 python\u2019s logging config. When I first started working with this <code>docker-compose.yml</code> i used the suggested <code>echo -e \"AIRFLOW_UID=$(id -u)\" &gt; .env</code> command which provided my user id (let\u2019s say it\u2019s 506 ) from my local machine and assigned it to the <code>AIRFLOW_UID</code> key. Now that i am working in the VM and have extended my <code>.env</code> file to include other information i figured i could just use a copy of the same file. Everything else works fine except airflow cannot write logs because the user in this virtual machine with user id 506 does not have permission to write to this <code>./logs/</code> directory. If you google this error i found \u2014 among a sea of almost right answers \u2014 that most of the solutions online are variations of \u201cchange the logs folder\u2019s permissions to 777\u201d meaning anyone can read, write, and execute the contents of the logs. That works. However, you don\u2019t really need everyone to be able to read and write \u2014 just this airflow user. Updating the UID on the VM\u2019s <code>.env</code> file worked perfectly without having to mess with the permissions.</p>","tags":["docker","airflow","python"]},{"location":"2022/01/how-to-solve-permission-error-from-airflow-official-docker-image/#how-i-learned","title":"how i learned","text":"<p>i kept getting permissions errors so i changed the <code>./logs/</code> directories permissions to 777 and ran <code>docker-compose up airflow-init</code> . Now that airflow was able to write logs i could run <code>ls -l logs/</code> and see that the owner of these logs was some user with id 506 which i recognized from the <code>.env</code> file. From there all i had to do was run <code>id -u</code> to find the correct user id (the id of the user i\u2019m logged in as in this VM) and update the <code>.env</code> file to match.</p>","tags":["docker","airflow","python"]},{"location":"2022/01/how-to-solve-permission-error-from-airflow-official-docker-image/#reference","title":"reference","text":"<p>the airflow documentation \u2192 airflow.apache.org/docs/apache-airflow/stable/start/docker.html</p> <p>this stackoverflow answer \u2192 stackoverflow.com/questions/67698656/cant-init-db-for-airflow-docker-compose-permission-denied/67704988</p> <p>this fantastic explanation of user and groups permissions \u2192 unix.stackexchange.com/questions/116070/granting-write-permissions-to-a-group-to-a-folder</p> <p>Share on  Share on  Share on </p>","tags":["docker","airflow","python"]},{"location":"2024/11/how-to-trigger-a-gh-action-only-if-the-issue-is-created-by-the-repo-owner/","title":"how to trigger a gh-action only if the issue is created by the repo owner","text":"","tags":["gh-actions","devops"]},{"location":"2024/11/how-to-trigger-a-gh-action-only-if-the-issue-is-created-by-the-repo-owner/#what-i-learned","title":"what i learned","text":"<p>you can add an <code>if</code> key to a job to conditionally run jobs. you also have a lot of metadata available in github actions regarding the event that triggered it and the repo it is on. </p> <p>put together you can add a condition like: .github/workflows/issue-to-md.yml<pre><code>...\njob:\n  job_name:\n    runs_on: ubuntu\n    if: ${{ github.event.issue.user.login == github.repository_owner }}\n...\n</code></pre></p>","tags":["gh-actions","devops"]},{"location":"2024/11/how-to-trigger-a-gh-action-only-if-the-issue-is-created-by-the-repo-owner/#how-i-learned","title":"how i learned","text":"<p>i have my <code>issue-to-md.yml</code> workflow to create til posts from issues on my repo but i realized that technically anyone could open an issue which would trigger the action and create a pull request. adding this condition ensures it'll only run if i am the one writing the issue. using the available metadata makes it reusable (i don't have to hardcode my own username). </p>","tags":["gh-actions","devops"]},{"location":"2024/11/how-to-trigger-a-gh-action-only-if-the-issue-is-created-by-the-repo-owner/#reference","title":"reference","text":"<ul> <li>gh actions docs: https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/using-conditions-to-control-job-execution</li> </ul> <p>Share on  Share on  Share on </p> Loading comments...","tags":["gh-actions","devops"]},{"location":"2024/11/i-just-set-up-alias-uvr--uv-run/","title":"i just set up `alias uvr = \"uv run\"`","text":"","tags":["uv","python"]},{"location":"2024/11/i-just-set-up-alias-uvr--uv-run/#what-i-learned","title":"what i learned","text":"<p>i just set up a new alias to save some time when using <code>uv</code></p> ~/.oh-my-zsh/custom/uv.zsh<pre><code>alias uvr = \"uv run\"\n</code></pre>","tags":["uv","python"]},{"location":"2024/11/i-just-set-up-alias-uvr--uv-run/#how-i-learned","title":"how i learned","text":"<p>i use streamlit a lot and i was playing around with <code>reflex</code> today and both tools have a <code>&lt;tool&gt; run</code> command. <code>marimo</code> does too. it's common enough in my workflow that it's becoming annoying to use <code>uv run</code> before these commands .</p>","tags":["uv","python"]},{"location":"2024/11/i-just-set-up-alias-uvr--uv-run/#reference","title":"reference","text":"<p>none really. checkout <code>reflex</code> though: https://reflex.dev</p> <p>Share on  Share on  Share on </p>","tags":["uv","python"]},{"location":"2024/11/running-sudo-commands-without-password-on-vps/","title":"running sudo commands without password on VPS","text":"","tags":["shell","vps"]},{"location":"2024/11/running-sudo-commands-without-password-on-vps/#what-i-learned","title":"what i learned","text":"<p>you can configure your VPS / server to be able to run <code>sudo</code> commands without being asked for your password. you just need to create a sudoers file. </p> <ul> <li> <p>first you have to create sudoers file <pre><code>sudo visudo -f /etc/sudoers.d/$USER\n</code></pre></p> <p>when i asked chatgpt for this i found you can just run <code>sudo visudo</code> and it\u2019ll open the sudoers file. </p> </li> <li> <p>now, let\u2019s say you have a user <code>app</code> that you want to be able to run <code>apt update</code> and <code>apt upgrade</code> without asking for <code>sudo</code> password. you need to add this line to your sudoers file <pre><code>app ALL=(ALL) NOPASSWD:/usr/bin/apt update, /usr/bin/apt upgrade\n</code></pre></p> </li> </ul>","tags":["shell","vps"]},{"location":"2024/11/running-sudo-commands-without-password-on-vps/#how-it-works","title":"how it works","text":"<ol> <li><code>app</code> - the username on the system </li> <li><code>ALL=(ALL)</code> - this means this rule to all hosts and allows acting as any user</li> <li><code>NOPASSWD</code> - no password</li> <li><code>/usr/bin/apt update</code> - you must pass the full path for the commands you want to run without a password. </li> </ol>","tags":["shell","vps"]},{"location":"2024/11/running-sudo-commands-without-password-on-vps/#how-i-learned","title":"how i learned","text":"<p>i recently found a tweet explaining all of this but i learned about it when setting a github action to push some code to my raspberry pi 5 running a FastAPI app and restarting the service. i needed to restart it using sudo but the github action would fail if prompted for it, i needed to run it without being asked for my password. </p>","tags":["shell","vps"]},{"location":"2024/11/running-sudo-commands-without-password-on-vps/#reference","title":"reference","text":"<ul> <li>the tweet in question: https://x.com/kkyrio/status/1856299320720363690</li> <li>chatgpt conversation: https://chatgpt.com/share/673366e9-aed8-8004-93e0-d72289fd3686</li> </ul> <p>Share on  Share on  Share on </p>","tags":["shell","vps"]},{"location":"2023/06/how-to-open-specific-channel-in-slack-app-using-stream-deck/","title":"how to open specific channel in Slack app using Stream Deck","text":"","tags":["streamdeck","automation"]},{"location":"2023/06/how-to-open-specific-channel-in-slack-app-using-stream-deck/#what-i-learned","title":"what i learned","text":"<p>you can open a specific channel/DM conversation on the slack app using the open website action on stream deck but pointing ot to <code>slack://channel?team=&lt;Workspace ID&gt;&amp;id=&lt;Channel / teammate ID&gt;</code>. </p> <p>finding the workspace's ID wasn't super straigh-forward but if you right click your own profile picture on slack and copy it's link you'll find 3 IDs. the first one is the workspace's ID. the second one is yours, if you use those two IDs you could open a direct message on slack directly from stream deck. </p> <p>once you have your workspace's ID all you need to do is <code>copy link</code> to the channels you're interested in having quick access to. these links do not include the workspace ID which is why you need to get it via a profile pic link.</p>","tags":["streamdeck","automation"]},{"location":"2023/06/how-to-open-specific-channel-in-slack-app-using-stream-deck/#how-i-learned","title":"how i learned","text":"<p>from adam.ac/blog</p>","tags":["streamdeck","automation"]},{"location":"2023/06/how-to-open-specific-channel-in-slack-app-using-stream-deck/#reference","title":"Reference","text":"<ul> <li>adam.ac/blog:    Stream Deck for Developers</li> </ul> <p>Share on  Share on  Share on </p>","tags":["streamdeck","automation"]},{"location":"2022/08/using-github-actions-to-produce-example-images-of-code/","title":"how to use gh-actions to produce example images of code","text":"","tags":["python","gh-actions","quarto","notion"]},{"location":"2022/08/using-github-actions-to-produce-example-images-of-code/#what-i-learned","title":"what i learned","text":"<p>I learned to chain a lot of small tools using GitHub Actions to produce ready-to-share images of code examples for social media (namely, instagram and twitter) from my phone. The steps, generally speaking, go as follows:</p> <ol> <li>Create a new page on a Notion Database. Probably will create a specific template for this, like I do with TIL\u2019s but it\u2019s not necessary.</li> <li>GitHub Action: Use my <code>markdownify-notion</code> python package to write the markdown version of this page and save it on a \u201cquarto project\u201d folder. This let\u2019s me use one general front-matter yaml file for all files rather than automate adding front matter to each file. I can still add specific front matter to files if I want to. (this TIL is an example of how this works - I\u2019m writing it on Notion on my phone.)</li> <li>GitHub Action: Use Quarto to render this markdown file <code>--to html</code> and save it on an \u201coutput\u201d directory. This will execute the code in the code cells and save the output inline.</li> <li>GitHub Action: Use <code>shot-scraper</code> to produce two files: a png screenshot and a pdf file. I\u2019m using <code>shot-scraper</code> for the PDF as well rather than using quarto because it\u2019s easier and I am not in need of customizing this pdf file at all just yet. I\u2019m creating it and saving it essentially just because I can, it\u2019s easy, and might find use for it later.</li> <li>GitHub Action: Once there are new png or pdf files in the \u201coutput\u201d directory, I then use <code>s3-credentials</code> to put those objects on a S3 bucket I also created using <code>s3-credentials</code> . This tool is fantastic s3-credentials.readthedocs.io</li> </ol> <p>This is how the final image looks like</p> <p></p>","tags":["python","gh-actions","quarto","notion"]},{"location":"2022/08/using-github-actions-to-produce-example-images-of-code/#how-i-learned","title":"how i learned","text":"<p>I wanted to checkout quarto for a while and in the last rstudio conference they announced it was finally at version 1.0 so I gave it a try. It\u2019s fairly straightforward but the documentation is clearly aimed at helping beginners and people that may not have any programming experience so a lot of the guides and tutorials and examples are for using Quarto within an editor like Rstudio or VS Code. It was hard to find examples of how to use it programmatically on your own - even the automating examples are using their GitHub Actions and services like Quarto publishing. This is actually great in general but if you need to do something custom they may not offer yet you need to figure it out on your own.</p>","tags":["python","gh-actions","quarto","notion"]},{"location":"2022/08/using-github-actions-to-produce-example-images-of-code/#reference","title":"reference","text":"<ul> <li> <p>Quarto:   quarto.org</p> </li> <li> <p>shot-scraper:   shot-scraper.datasette.io/en/stable/</p> </li> <li> <p>s3-credentials:   s3-credentials.readthedocs.io/en/stable/</p> </li> <li> <p>markdownify-notion:   github.com/chekos/markdownify-notion</p> </li> </ul> <p>Share on  Share on  Share on </p>","tags":["python","gh-actions","quarto","notion"]},{"location":"2024/11/using-typer-and-uv-to-run-a-script-with-inline-dependencies/","title":"using typer and uv to run a script with inline dependencies","text":"","tags":["python","uv","micro-packages"]},{"location":"2024/11/using-typer-and-uv-to-run-a-script-with-inline-dependencies/#what-i-learned","title":"what i learned","text":"<p>because <code>uv</code> supports running scripts with dependencies declared in inline metadata and <code>typer</code> can turn any function into a cli you can put both of them together and build some really powerful small utilities. all you need is to define a function and wrap it in <code>typer.run()</code> in a script with <code>typer</code> as a dependency in the inline metadata.</p> <p>after some iterations, this is the final script (so far):</p> issue-to-md.py<pre><code># /// script\n# dependencies = [\n#   \"typer\",\n#   \"rich\",\n#   \"pyyaml\",\n# ]\n# ///\n\nimport json\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\nfrom zoneinfo import ZoneInfo\n\nimport typer\nimport yaml\nfrom rich import print\nfrom typing_extensions import Annotated\n\n\ndef generate_post_from_issue(\n    issue_title: Annotated[str, typer.Option(\"--title\", \"-t\")],\n    issue_body: Annotated[str, typer.Option(\"--body\", \"-b\")],\n    issue_labels: Annotated[str, typer.Option(\"--labels\", \"-l\")],\n    issue_created_at: Annotated[str, typer.Option(\"--created-at\", \"-c\")],\n    base_dir: Annotated[str, typer.Option(\"--base-dir\", \"-d\")] = \"blog/posts\",\n):\n    # Convert labels to a list of tags\n    tags = [label[\"name\"] for label in json.loads(issue_labels)]\n\n    # Convert ISSUE_CREATED_AT to PST and format as YYYY-MM-DD\n    utc_time = datetime.strptime(issue_created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n    pst_time = utc_time.astimezone(ZoneInfo(\"America/Los_Angeles\"))\n    created_at_pst = pst_time.date()\n\n    # Extract the category from the part of the title before the first colon, default to \"project\" if none\n    category = (\n        issue_title.split(\":\")[0].strip().lower() if \":\" in issue_title else \"project\"\n    )\n\n    # Extract the title content after the first colon\n    title = (\n        issue_title.split(\":\", 1)[1].strip()\n        if \":\" in issue_title\n        else issue_title.strip()\n    )\n\n    # Determine directory based on category\n    dir_path = Path(base_dir) / (\"til\" if category == \"til\" else \"\")\n    dir_path.mkdir(parents=True, exist_ok=True)\n\n    # Generate a slugified version of the title for the filename\n    slug = re.sub(r\"[^a-z0-9]+\", \"-\", title.lower()).strip(\"-\")\n\n    # Create the front matter dictionary\n    front_matter = {\n        \"title\": title,\n        \"date\": created_at_pst,\n        \"categories\": [category],\n        \"tags\": tags,\n    }\n\n    # Prepare YAML front matter and issue body\n    yaml_front_matter = yaml.dump(front_matter, default_flow_style=False)\n    content = f\"---\\n{yaml_front_matter}---\\n\\n{issue_body}\"\n\n    # Define filename\n    filename = dir_path / f\"{slug}.md\"\n\n    # Write content to file\n    filename.write_text(content, encoding=\"utf-8\")\n\n    print(f\"Markdown file created: {filename}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(generate_post_from_issue)\n</code></pre> <p>feels like a micro-package.</p>","tags":["python","uv","micro-packages"]},{"location":"2024/11/using-typer-and-uv-to-run-a-script-with-inline-dependencies/#how-i-learned","title":"how i learned","text":"<p>in another til i described a github action that when triggered (by the creation of a new issue in a github repo) creates a markdown file using the title, created at, and labels of the issue for front-matter metadata. the action then also creates a new branch and a pull request but that's not important right now.</p> <p>the logic for that first part is encapsulated here:</p> .github/workflows/issue-to-md.yml<pre><code>- name: Generate Post from Issue\n    env:\n      ISSUE_NUMBER: ${{ github.event.issue.number }}\n      ISSUE_TITLE: ${{ github.event.issue.title }}\n      ISSUE_BODY: ${{ github.event.issue.body }}\n      ISSUE_LABELS: ${{ toJson(github.event.issue.labels) }}\n      ISSUE_CREATED_AT: ${{ github.event.issue.created_at }}\n    run: |\n      # Convert labels to a list of tags\n      TAGS=$(echo $ISSUE_LABELS | jq -r '.[] | .name' | paste -sd, -)\n\n      # Convert ISSUE_CREATED_AT to PST and format as YYYY-MM-DD\n      CREATED_AT_PST=$(TZ=\"America/Los_Angeles\" date -d \"${ISSUE_CREATED_AT}\" +\"%Y-%m-%d\")\n\n      # Extract the category from the part of the title before the first colon, default to \"project\" if none\n      CATEGORY=$(echo \"$ISSUE_TITLE\" | awk -F: '{print $1}' | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')\n      if [ -z \"$CATEGORY\" ]; then\n        CATEGORY=\"project\"\n      fi\n\n      # Extract the title content after the first colon\n      TITLE=$(echo \"$ISSUE_TITLE\" | sed 's/^[^:]*: *//')\n\n      # Determine directory based on category\n      if [ \"$CATEGORY\" = \"til\" ]; then\n        DIR=\"blog/posts/til\"\n      else\n        DIR=\"blog/posts\"\n      fi\n      echo $DIR &gt;&gt; $GITHUB_STEP_SUMMARY\n      echo $CATEGORY &gt;&gt; $GITHUB_STEP_SUMMARY\n\n      # Generate a slugified version of the title for the filename\n      SLUG=$(echo \"$TITLE\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '-' | sed 's/^-//;s/-$//')\n\n      echo $SLUG &gt;&gt; $GITHUB_STEP_SUMMARY\n\n      # Create the front matter with category, tags, and formatted date\n      FRONT_MATTER=\"---\\ntitle: \\\"$TITLE\\\"\\ndate: ${CREATED_AT_PST}\\ncategories: [${CATEGORY}]\\ntags: [${TAGS}]\\n---\"\n\n      # Prepare content for markdown file\n      CONTENT=\"$FRONT_MATTER\\n\\n$ISSUE_BODY\"\n\n      # Save the content to a markdown file\n      FILENAME=\"${DIR}/${SLUG}.md\"\n      echo $FILENAME &gt;&gt; $GITHUB_STEP_SUMMARY\n      echo -e \"$CONTENT\" &gt; \"$FILENAME\"\n</code></pre> <p>it's not super complicated but i don't know write many bash scripts so i was depending on chatgpt to get this right. any modifications or additions would need me to ask chatgpt as well. the logic itself is kind of straight forward:</p> <ol> <li>grab the title, if there's a ':' split it, the first half is the category the second is the title.</li> <li>grab the labels object, grab only the label names, those are the tags in the front matter.</li> <li>grab the issue body and use that as the post.</li> <li>grab the created at string, make it a date, move it to PST instead of UTC, grab the date, add that to the front matter.</li> </ol> <p>i'm using material for mkdocs for these and i saw that it's suggested to have a date.created and a date.updated metadata rather than just date. sounds easy enough but honestly, the idea of messing with this bash script to create that nested thing was not my favorite despite of how easy it may actually be.</p> <p>so i figured it'd be better to just move this to python and ideally still run it as a command / cli type thing but i also didn't want to create a new package. i just wanted a small script to include in my blog repo.</p> <p>python comes with <code>argparse</code> so that could have been the end of it but i also don't use much <code>argparse</code> and the idea was to create something small and quick but also something i can build and iterate on myself without much extra help (ai or not). so <code>typer</code> it is. i also didn't necessarily want to add <code>typer</code> and any other dependencies to the blog repo since those are not needed for the blog itself. i recently heard an episode of <code>python bytes</code> where they mention that <code>uv</code> supports dev dependencies and they talked a bit about how some dependency groups are building upon the base one and others are just completely independent. that sounded promising but when i went to the <code>uv</code> docs i was reminded that it can run scripts and use inline metadata so that just solved it right there and then.</p> <p>it took me longer to write this til than write that script.</p>","tags":["python","uv","micro-packages"]},{"location":"2024/11/using-typer-and-uv-to-run-a-script-with-inline-dependencies/#reference","title":"reference","text":"<ul> <li>uv docs: https://docs.astral.sh/uv/guides/scripts/#running-a-script-with-dependencies</li> <li>python docs: https://packaging.python.org/en/latest/specifications/inline-script-metadata/#inline-script-metadata</li> <li>python bytes episodes discussing dependency groups:</li> <li>#407: Back to the future, destination 3.14</li> <li>#406: What's on Django TV tonight?</li> </ul> <p>Share on  Share on  Share on </p>","tags":["python","uv","micro-packages"]},{"location":"archive/2025/","title":"July 2025","text":""},{"location":"archive/2024/","title":"November 2024","text":""},{"location":"archive/2023/","title":"June 2023","text":""},{"location":"archive/2022/","title":"December 2022","text":""},{"location":"archive/2021/","title":"September 2021","text":""},{"location":"archive/2020/","title":"June 2020","text":""},{"location":"category/reflection/","title":"reflection","text":""},{"location":"category/ai/","title":"ai","text":""},{"location":"category/technology/","title":"technology","text":""},{"location":"category/til/","title":"til","text":""},{"location":"category/data-projects/","title":"data projects","text":""},{"location":"category/personal/","title":"personal","text":""},{"location":"page/2/","title":"\u00a1hola! \ud83d\udc4b\ud83c\udffc","text":""},{"location":"page/3/","title":"\u00a1hola! \ud83d\udc4b\ud83c\udffc","text":""},{"location":"category/til/page/2/","title":"til","text":""},{"location":"tags/","title":"Tag index","text":""},{"location":"tags/#ai-tools","title":"ai-tools","text":"<ul> <li>Of Brasas &amp; Nube</li> </ul>"},{"location":"tags/#airflow","title":"airflow","text":"<ul> <li>How To Solve Permission Error From Airflow Official Docker Image</li> </ul>"},{"location":"tags/#automation","title":"automation","text":"<ul> <li>How To Create An Alias In The gh-cli</li> <li>how to open specific channel in Slack app using Stream Deck</li> </ul>"},{"location":"tags/#aws","title":"aws","text":"<ul> <li>Setting Up Ffmpeg As Lambda Layer</li> </ul>"},{"location":"tags/#datasette","title":"datasette","text":"<ul> <li>Haciendo datos abiertos m\u00e1s accesibles con datasette</li> </ul>"},{"location":"tags/#dataviz","title":"dataviz","text":"<ul> <li>Alem\u00e1n: Alemalacra Alemalandro Alemaliya</li> </ul>"},{"location":"tags/#development","title":"development","text":"<ul> <li>Of Brasas &amp; Nube</li> </ul>"},{"location":"tags/#devops","title":"devops","text":"<ul> <li>how to trigger a gh-action only if the issue is created by the repo owner</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>How To Solve Permission Error From Airflow Official Docker Image</li> </ul>"},{"location":"tags/#docs","title":"docs","text":"<ul> <li>About Myst-Parser, Es Markdown Pero Rst</li> </ul>"},{"location":"tags/#ffmpeg","title":"ffmpeg","text":"<ul> <li>Setting Up Ffmpeg As Lambda Layer</li> </ul>"},{"location":"tags/#gh","title":"gh","text":"<ul> <li>How To Create An Alias In The gh-cli</li> </ul>"},{"location":"tags/#gh-actions","title":"gh-actions","text":"<ul> <li>how to trigger a gh-action only if the issue is created by the repo owner</li> <li>Using Github Actions To Produce Example Images Of Code</li> </ul>"},{"location":"tags/#hip-hop","title":"hip hop","text":"<ul> <li>Alem\u00e1n: Alemalacra Alemalandro Alemaliya</li> </ul>"},{"location":"tags/#jekyll","title":"jekyll","text":"<ul> <li>Escaping Liquid Tags With {% Raw %}</li> <li>About Jekyll-Archives</li> <li>About Liquid Tags</li> </ul>"},{"location":"tags/#jq","title":"jq","text":"<ul> <li>Jq [] Syntax</li> <li>How To Copy Json Straight To Clipboard From The Terminal</li> </ul>"},{"location":"tags/#liquid-tags","title":"liquid tags","text":"<ul> <li>Escaping Liquid Tags With {% Raw %}</li> <li>About Liquid Tags</li> </ul>"},{"location":"tags/#micro-packages","title":"micro-packages","text":"<ul> <li>using typer and uv to run a script with inline dependencies</li> </ul>"},{"location":"tags/#notion","title":"notion","text":"<ul> <li>Using Github Actions To Produce Example Images Of Code</li> </ul>"},{"location":"tags/#parallel-time","title":"parallel-time","text":"<ul> <li>Of Brasas &amp; Nube</li> </ul>"},{"location":"tags/#productivity","title":"productivity","text":"<ul> <li>Of Brasas &amp; Nube</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>Haciendo datos abiertos m\u00e1s accesibles con datasette</li> <li>How To Solve Permission Error From Airflow Official Docker Image</li> <li>i just set up <code>alias uvr = \"uv run\"</code></li> <li>Using Github Actions To Produce Example Images Of Code</li> <li>using typer and uv to run a script with inline dependencies</li> </ul>"},{"location":"tags/#quarto","title":"quarto","text":"<ul> <li>Using Github Actions To Produce Example Images Of Code</li> </ul>"},{"location":"tags/#serverless","title":"serverless","text":"<ul> <li>Setting Up Ffmpeg As Lambda Layer</li> </ul>"},{"location":"tags/#shell","title":"shell","text":"<ul> <li>Jq [] Syntax</li> <li>How To Copy Json Straight To Clipboard From The Terminal</li> <li>How To Execute A Shell Script In The Current Shell</li> <li>Running sudo commands without password on VPS</li> </ul>"},{"location":"tags/#sphinx","title":"sphinx","text":"<ul> <li>About Myst-Parser, Es Markdown Pero Rst</li> </ul>"},{"location":"tags/#sql","title":"sql","text":"<ul> <li>Haciendo datos abiertos m\u00e1s accesibles con datasette</li> </ul>"},{"location":"tags/#streamdeck","title":"streamdeck","text":"<ul> <li>how to open specific channel in Slack app using Stream Deck</li> </ul>"},{"location":"tags/#unix","title":"unix","text":"<ul> <li>You Can'T Use Special Characters In Unix Commands If You Use Single-Quotes</li> </ul>"},{"location":"tags/#uv","title":"uv","text":"<ul> <li>i just set up <code>alias uvr = \"uv run\"</code></li> <li>using typer and uv to run a script with inline dependencies</li> </ul>"},{"location":"tags/#vps","title":"vps","text":"<ul> <li>Running sudo commands without password on VPS</li> </ul>"}]}